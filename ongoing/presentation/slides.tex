\documentclass{beamer}

\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{stackengine}

\setcounter{section}{-1}


\newcommand{\Disp}[1]{\begin{align} #1 \end{align}}

\renewcommand{\H}{\dagger}
\newcommand{\Tr}{\intercal}
\newcommand{\NT}{\notag}
\renewcommand{\d}{\delta}
\newcommand{\f}{\varphi}
\renewcommand{\th}{\vartheta}
\newcommand{\I}{\item}

\newcommand{\MB}[1]{\mathbb{#1}}
\newcommand{\MC}[1]{\mathcal{#1}}
\newcommand{\MF}[1]{\mathfrak{#1}}
\newcommand{\RM}[1]{\mathrm{#1}}
\newcommand{\R}[1]{\sqrt{#1}}
\newcommand{\F}[1]{\frac{#1}}
\newcommand{\V}[1]{\stackunder[1.2pt]{$#1$}{\rule{.9ex}{.075ex}}}
\newcommand{\M}[1]{\V{\V{#1}}}
\newcommand{\T}[1]{\tilde{#1}}
\newcommand{\IP}[1]{\langle #1 \rangle}

\title{Compressive Channel Sensing of Multipath Millimeter Channel via a Beamspace Dantzig Selector and its Probabilitic Performance Bound}
\author{Presenter: Tzu-Yu Jeng \\ Advisor: Prof.\ H.J.\ Su}
\date{Jan.\ 9, 2019}
\institute{Graduate Instute of Commnication Engineering, NTU}
 
\begin{document}
 
\frame{\titlepage}
 
\begin{frame}
\frametitle{Sample frame title}
This is a text in the first frame. This is a text in the first frame. This is a text in the first frame.
\end{frame}

\begin{frame}
\frametitle{Notation}
\begin{itemize}
\I \(\V{a}\) denotes vectors
\I \(\M{A}\) denotes matrices
\I \(\MB{K}\) refers to either \(\MB{R}\) or \(\MB{C}\).
\I \(\MB{V}_{\MB{K}} (N)\) is the Hilbert space \(\MB{K}^N\) over \(\MB{K}\)
\I \(\MB{M}_{\MB{K}} (M,N)\) is the collection of \(M\) by \(N\) matrices.
\I \(\|\V{a}\|_p\) denotes the \(\ell_p\)-norm \(\V{a}\)
\end{itemize}

\end{frame}



\begin{frame}
\frametitle{Channel Model}
\begin{itemize}
\I The response of uniform linear array is
\Disp{
\V{a} (\psi')
=\sum_{n=1}^{N} \RM{e}^{n \psi' i} \V{u}_n
\in \MB{V}_\MB{C} (N_H)
}
\I Let the angle of departure and arrival absorb the factor
\Disp{
\f_l =\dfrac{d_{\RM{arr}}} {\lambda} \sin \f_l', \quad \th_l =\dfrac{d_{\RM{arr}}} {\lambda} \sin \th_l'
}
\I The channel matrix is
\Disp{
=\sum_{l=0}^L \alpha_l \V{a} (\f_l) \V{a} (\th_l)^\H
\in \MB{M}_\MB{C} (N_H, N_H)
}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Precoders Model}
\begin{itemize}
\I We consider the hybrid configuration at both transmitter and receiver end.
In the transmitter end, there are digital precoder \(F_B\) and analog precoder \(F_R\).
In the receiver end, there are digital combiner \(W_B\) and analog combiner \(W_R\).
\Disp{
\M{F}_B \in &\MB{M}_{\MB{C}} (N_R, N_Y) \\
\M{F}_R \in &\MB{M}_{\MB{C}} (N_H, N_R) \\
\M{W}_R \in &\MB{M}_{\MB{C}} (N_R, N_H) \\
\M{W}_B \in &\MB{M}_{\MB{C}} (N_Y, N_R)
}
\I Recall the constraint of magnitude for analog precoders:
\Disp{
|\M{F}_R (n_h, n_r)| =&1, \quad |\M{W}_R (n_r, n_h)| =1, \\
n_h =&1, \dotsc N_H, \quad n_r =1, \dotsc N_R \NT
}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Effective Channel}
\begin{itemize}
\I We have the effective channel \(Y =W_B W_R ( H F_R F_B +Z ) \in \MB{M}_{\MB{C}} (N_Y, N_R)\)
\I We may estimate \(\M{Y} (:,n_y)\) with \(\M{W}_B \M{W}_R [ \M{H} \M{F}_R \M{F}_B \V{u}_{n_y} +\V{z} ]\), \(n_y =1, 2, \dotsc, N_Y\), using pilog signal as unit vectors \(\V{u}_{n_y}\).
\I It remains to recover \(\M{H}\) with knowledge of \(\M{Y}\), while \(\M{W}_R\), \(\M{W}_B\), \(\M{F}_R\), and \(\M{F}_B\) are in our control
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Vectorization}
Previous literature usually approaches the problem as
\Disp{
\V{h}
:= &\RM{vec} (\M{H})
\in \MB{V}_{\MB{C}} (N_g) \\
\V{y}
:= &\RM{vec} (\M{Y})
\in \MB{V}_{\MB{C}} (N_y) \\
\V{z}^\star
:= &\RM{vec} (\M{Z})
\in \MB{V}_{\MB{C}} (N_y) \\
\M{P}^\star
:= &(\M{F}_R^\Tr \M{F}_B^\Tr) \otimes (\M{W}_B \M{W}_R)
\in \MB{M}_{\MB{C}} (N_y, N_g).
}
So that \(\V{y} =\M{P}^\star \V{h} +\V{z}^\star\)
\end{frame}

\begin{frame}
\frametitle{Vectorization}
But if we write \( \M{G} =\M{K}^\H \M{H} \M{K} \), i.e.\ the beamspace (i.e., spatial frequency domain) representation of \(\M{H}\), then
\Disp{
\M{Y}
:=\M{W}_B \M{W}_R \M{K} ( \M{G} \M{K}^\H \M{F}_R \M{F}_B +\M{K}^\H \M{Z} )
\in \MB{M}_{\MB{C}} (N_Y, N_Y) \\
\M{P}
:=(\M{F}_B^\Tr \M{F}_R^\Tr K^\ast) \otimes (\M{W}_B \M{W}_R \M{K})
\in \MB{M}_{\MB{C}} (N_Y^2, N_H^2)
}
where accordingly \(\V{g} := \RM{vec} (\M{G}) \in \MB{M}_{\MB{C}} (N_g)\), \(\V{z} := \RM{vec} (\M{K}^\H \M{Z}) \in \MB{V}_{\MB{C}} (N_y)\),
and for short we set \(N_g =N_H^2\) and \(N_y =N_Y^2\),
so that
\Disp{
\V{y}
=\M{P} \V{g} +\V{z}
}

\end{frame}


\begin{frame}
\frametitle{Lemma: \(g\) is Almost Sparse}
\(\forall \f\), \(\V{a} (\f)\) is almost-\(L\)-sparse with \(\ell_1\)-residue \(R(N_H, L)\) to be
\Disp{
R
\leq 2 N_H \log \F{2\pi N_H}{L}.
}
\end{frame}

\begin{frame}
\frametitle{Proof (1/3)}
\Disp{
D (\f')
:=&\left| \sum_{n=0}^{N_H-1} \RM{e}^{i n \f'} \right|
=\F{|\sin (N_H \f'/2)|}{|\sin (\f' /2)|} \\
\left| x -\F{x^3}{6} \right|
\leq &\sin x, \quad -\pi \leq x \leq \pi \\
\left| D (\f') \right|
= &\F{48}{|\f'^2 -24| |\f'|}
}
\begin{enumerate}
\I Definition and arrangement
\I Can be shown with basic calculus
\I Plug into \(D\)
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Proof (2/3)}
\Disp{
\M{K}^\H \V{a}(\f) (k)
=&\F{1}{N_H} D \left( \f -\F{2 \pi k} {N_H} \right) \\
R(\eta)
:=&\F{1}{N_H} \sum_{n_H =s}^{N_H -1} D \left( \eta +\F{2 \pi n_H} {N_H} \right) \\
R(\eta) -\F{2\pi} {N_H}
\leq &\F{1}{N_H} N_H \int_{\f'=2\pi L/N_H}^{2\pi} |D(\f')| d \f'
}
\begin{enumerate}
\I Straightforward
\I Definition
\I Approximation of rectangle to integral
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Proof (3/3)}
\Disp{
R(\f)
\leq &\F{1}{2\pi} \int_{\f'=2\pi L/N_H}^{2\pi} \F{48}{(24 -\f'^2) \f'} d \f'
+\F{2\pi} {N_H} \NT \\
=&\F{1}{\pi} \log \F{2\pi N_H}{L}
-\F{1}{N_H} \log \F{4\pi^2 -24} {L^2/N_H^2 -24}
+\F{2\pi} {N_H} \\
\leq &\F{1}{\pi} \log \F{N_H}{L}.
}
\begin{enumerate}
\I Plugging in
\I Calculation
\I Dropping terms
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Lemma (Tentative): \(\V{g}\) is Almost-\(L\)-Sparse}
Let \(\f\) and \(\th\) be uniformly, independently distributed in \([0,2\pi)\).
Then \(\V{g}\) is almost-\(L\)-sparse with \(\ell_1\)-residue \(R(N_H, L)\) to be
\Disp{
R
\leq \F{L}{\pi^2} \left( \log \F{N_H}{L} \right)^2
}
\end{frame}



\begin{frame}
\frametitle{Lemma}
Let \(P\) be i.i.d.\ Radmacher, as defined in Lemma ().
Then \(P\) is RIP according to \(\d\) for at least probability
\Disp{
1 -2 \left( \F{12}{\d} \right)^s \exp \left[ - \left( \F{\epsilon^2}{4} -\F{\epsilon^3}{6} \right) \F{\d}{2} N_p \right]
}
{\small Source: R Baraniuk, M Davenport, R DeVore, and M Wakin (2008), ``A Simple Proof of the Restricted Isometry Property for Random Matrices'', \textit{Constructive Approximation} \textbf{28}: 253–263 }
\end{frame}

\begin{frame}
\frametitle{Definitions}
\begin{itemize}
\I Denote \(d =\hat{g} -g\)
\I \(\MC{A}\) be the largest \(s\) position of \(g\).
\I \(\MC{B}\) be the largest \(s\) position of \(d_{T(N_g) -\MC{A}}\).
\I \(\MC{C} =T(N_g) -\MC{A}\), i.e., the complementary index set of \(\MC{A}\).
\I Denote as \(\M{A}_{\MC{T}}\) the columns of \(\M{A}\) having indices in \(\MC{T}\).
\I we say \(\V{x}\) is almost-\(s\)-sparse with \(\ell_1\)-residue \(R\), where \(s \ll N\), if
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Lemmata}
\begin{itemize}
\I \( \|\V{d}_{\MC{C}}\|_1 \leq \|\V{d}_{\MC{A}}\|_1 +2\|\V{g}_{C}\|_1 \)
\I \( |\IP{ \V{z}, \M{P}(:, n_g) }| \leq 2 \R{\log N_g}, \quad n_g =1, \dotsc, N_g. \) for overwhelming probability
\I \( \|\M{P}^\H \M{P} \V{d}\|_\infty \leq 4 \R{\log N_g}, \quad n_g =1, \dotsc, N_g \) for overwhelming probability
\end{itemize}
{\small Source: E Cand\`es and T Tao (2007), ``The Dantzig Selector: Statistical Estimation when \(p\) is Much Larger than \(n\)'', \textit{The Annals of Statistics}, Vol.35, No.6}
\end{frame}


\begin{frame}
\frametitle{Lemmata}
\begin{itemize}
\I \( \|\V{d}_{\MC{AB}}\|_2 \leq \dfrac{1}{1-\d_{2s}} \|P_{\MC{A}\MC{B}}^\Tr P d\|_2 +\dfrac{\d_{3s}}{(1-\d_{2s}) \R{s}} \|d_{C}\|_1 \) for overwhelming probability
\I \( \|\V{d}\|_2^2 \leq \|\V{d}_{\MC{A}\MC{B}}\|_2^2 +\dfrac{1}{s} \|\V{d}_{\MC{C}}\|_1^2 \)
\end{itemize}
{\small Source: E Cand\`es and T Tao (2007), ``The Dantzig Selector: Statistical Estimation when \(p\) is Much Larger than \(n\)'', \textit{The Annals of Statistics}, Vol.35, No.6}
\end{frame}

\begin{frame}
\frametitle{The Main Bound}
Let \(\V{y}\), \(\M{P}\), \(\V{g}\), \(\hat{\V{g}}\), \(\V{d}\) be defined as above, and set \(s=L\), then
\Disp{
\|\V{d}\|_2
\leq \F{4}{\pi^4} \d_{3L}^4 (1-2\d_{2L}) L^4 ( \log N_H )^4
}
\end{frame}

\begin{frame}
\frametitle{Proof (1/4)}
\Disp{
\|\M{P}_{\MC{A}\MC{B}}^\Tr \M{P} \V{d}\|_2
\leq &\|\M{P}^\Tr \M{P} \V{d}\|_2 \NT \\
\leq &\R{N_g} \|\M{P}^\Tr \M{P} \V{d}\|_\infty \NT \\
\leq &4 \R{ N_g \log N_g }
}
\begin{enumerate}
\I Inclusion of larger support
\I \(\ell_p\) norm inequality
\I Plug in the Lemma for \(\|\M{P}^\Tr \M{P} \V{d}\|_2\)
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Proof (2/4)}
\Disp{
\| \V{d}_{A} \|_1
\leq &\R{L} \| \V{d}_{A} \|_2 \NT \\
\leq &\R{L} \| \V{d}_{\MC{AB}} \|_2 \NT \\
\leq &\F {\d_{3L}} {1-\d_{2L}} \| \V{d}_{\MC{C}} \|_1 +\F {4} {1-\d_{2L}} \R{L N_g \log N_g} 
}
\begin{enumerate}
\I \(\ell_p\) norm inequality
\I Inclusion of larger support
\I Plug in the Lemma for \(\|\V{d}_{\MC{A}\MC{B}}\|_2\)
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Proof (3/4)}
\Disp{
\| \V{d}_{\MC{C}} \|_1
\leq &\| \V{d}_{\MC{A}} \|_1 +2 \| \V{g}_{\MC{C}} \|_1 \NT \\
\leq &\F {\d_{3L}} {1-\d_{2L}} \| \V{d}_{\MC{C}} \|_1
+2 \| \V{g}_{\MC{C}} \|_1
+\F {4} {1-\d_{2L}} \R{L N_g \log N_g} \\
\| \V{d}_{\MC{C}} \|_1
\leq &2 \F{1-\d_{2L}} {1 -\d_{2L} -\d_{3L}} \| \V{g}_{\MC{C}} \|_1
+\F {4} {1-\d_{2L} -\d_{3L}} \R{L N_g \log N_g}
}
\begin{enumerate}
\I Copy the Lemma for \(\| \V{d}_{\MC{C}} \|_1\)
\I Plug in the previous eqn.\ for \(\| \V{d}_{\MC{A}} \|_1\)
\I Cancellation, simplification
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Proof (4/4)}
\Disp{
\| \V{d} \|_2^2
\leq &\|\V{d}_{\MC{A}\MC{B}}\|_2^2 +\F{1}{L} \|\V{d}_{\MC{C}}\|_1^2 \NT \\
\leq & \left( \F{\d_{3L}^2} {L^2 (1-\d_{2L})^2} +\F{1}{L} \right) \|\V{d}_{\MC{C}}\|_1^2 \NT \\
&\quad +\F{8 \d_{3L}} {L (1-\d_{2L})^2} \R{N_g \log N_g} \|\V{d}_{\MC{C}}\|_1 \NT \\
&\quad +\F{16}{(1-\d_{2L})^2} N_g \log N_g \\
\|\V{d}\|_2^2
\leq &\F{4}{\pi^4} \d_{3L}^4 (1-2\d_{2L}) L^4 ( \log N_H )^4
}
\begin{enumerate}
\I Copy the Lemma for \(\| \V{d} \|_2^2\)
\I Plug in the Lemma for \(\|\V{d}_{\MC{A}\MC{B}}\|_2\)
\I Approximation with \(1 \ll L \ll N_g\), and \(\d_{2L} <\d_{3L} \ll 1\)
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Proof}
\Disp{
}
\begin{enumerate}
\I 
\I 
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Proof}
\Disp{
}
\begin{enumerate}
\I 
\I 
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Proof}
\Disp{
}
\begin{enumerate}
\I 
\I 
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Proof}
\Disp{
}
\begin{enumerate}
\I 
\I 
\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Proof}
\Disp{
}
\begin{enumerate}
\I 
\I 
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Conclusion}
\begin{enumerate}
\I We are concerned with the problem of effective and estimation of the millimeter wave channel by exploiting its sparsity
\I We adopt The Dantzig Selector (DS) rather than Orthogonal Matching Pursuit (OMP), like many of past literature, as the estimation algorithm.
\I We have derived the generalization of DS for complex vectors
\I We proceed the estimation algorithm in the spatial frequency domain, assuming the virtual channel model according to uniform linear array response, 
\I We proved explicitly that the probability that the expected square error is bounded is overwhelming, where the bounding constant is an explicit function of the number of path
\I It remains to transform DS into Second Order Cone Problem, and finding the Lagrangian and dual problem, do the simulation and compare the theoretical bound
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{References}
\begin{enumerate}
{ \small
\item W U Bajwa, J Haupt, G Raz, and R Nowak (2008), ``Compressed Channel Sensing'', 2008 42nd Annual Conference on Information Sciences and Systems.
\item R Baraniuk, M Davenport, R DeVore, and M Wakin (2008), ``A Simple Proof of the Restricted Isometry Property for Random Matrices'', \textit{Constructive Approximation} \textbf{28}: 253–263
\item S Boyd, L Vandenberghe (2004), \textit{Convex Optimization}. Cambridge U.\ Press.
\item E J Cand\`es and J Romberg (2005), ``\(\ell_1\)-MAGIC: Recovery of Sparse Signals via Convex Programming'', Retrieved from \url{https://statweb.stanford.edu/~candes/l1magic/downloads/l1magic.pdf}.
\item E J Cand\`es and T Tao (2005), ``Decoding by Linear Programming'', \textit{IEEE Transactions on Information Theory}, Vol.51, No.12.
\item E Cand\`es and T Tao (2007), ``The Dantzig Selector: Statistical Estimation when \(p\) is Much Larger than \(n\)'', \textit{The Annals of Statistics}, Vol.35, No.6.
}
\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Proof}
\Disp{
}
\begin{enumerate}
\I 
\I 
\end{enumerate}
\end{frame}



% % % % % % % % % % % % % % % % % % % % 

\end{document}

\begin{frame}
\frametitle{Definition}
For \(\MC{T}, \MC{T}' \subset \{1, \dotsc, N_p\}\), define the \(s, s'\)-restricted orthogonality constant \(\tau_{s,s'} >0\) to be the smallest such that
%
\Disp{
| \IP{ P_{\MC{T}} h, P_{\MC{T}'} h' } |
\leq \tau_{s, s'} \cdot \| h \|_2 \|h'\|_2
}
\end{frame}

\begin{frame}
\frametitle{Lemma: \(\d_{s+s'}\) Can Bound \(\tau_{s, s'}\)}
Let \(P\) be given.
Then \(\tau_{s, s'}\) is bounded in both direction as follows,
\Disp{
\d_{s+s'} -\max (\{\d_s, \d_{s'}\})
\leq &\tau_{s, s'} \\
\leq &\d_{s+s'}
}
{\small Source: E J Cand\`es and T Tao (2005), ``Decoding by Linear Programming'', \textit{IEEE Transactions on Information Theory}, Vol.51, No.12. }
\end{frame}
