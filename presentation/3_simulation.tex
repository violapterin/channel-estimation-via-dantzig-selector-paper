% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {Linear Programming}
{
\I DS is equivalent to a linear program, which allows more efficient convex programming methods to be used.
(Variables with tilde denote the real representation of complex matrices and vectors.)
\Disp {
\NC \Hat {\T {\V {g}}}, \Hat {\T {\V {f}}}
\LA \NC \startcases
   \NC \Min {\T {\V {g}}', \T {\V {f}}'}  \MC \IP {\V {1}, \T {\V {f}}'} \NR
   \NC \Rm {subject} \; \Rm {to} \MC \T {\V {g}}' \preceq \T {\V {f}}',
   \quad \MC - \T {\V {g}}' \preceq \T {\V {f}}' \NR
   \NC \MC \T {\M {P}}^\Adj \T {\M {P}} \T {\V {g}}' \preceq \T {\M {P}}^\Adj \T {\V {y}} + \g_{\Ss {DS}} \V {1} \NR
   \NC \MC - \T {\M {P}}^\Adj \T {\M {P}} \T {\V {g}}' \preceq - \T {\M {P}}^\Adj \T {\V {y}} + \g_{\Ss {DS}} \V {1} \NR
\stopcases \NR
}
\I Output \m {\Hat {\M {H}} \LA \M {K} \Rm {vec}^{-1} \SB {\Hat {\T {\V {g}}}} \M {K}^\Adj}
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {Performance Metric}
{
\I Let the \m {\T {\chi}}, the relative 2-norm error, be the performance metric.

\Disp {
\NC \T {\chi}
=\NC \RB {
   \F {\log_2 {\VNm {\V {h} -\Hat {\V {h}}} _2}}
   {\log_2 {\VNm {\V {h}}_2}}
} _{\Ss {avg}}, \NR
}

\I Also define \m {\chi} by plugging in \m {\V {d}} into \m {\T {\chi}}.
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {\m {1/\s} vs \m {\T {\chi}}, assorted, \m {N_H=16=2N_B}}
{
\blank [big]
\externalfigure [assorted-narrow-medium-error.png] [width=11cm]
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {\m {1/\s} vs \m {\T {\chi}}, assorted, \m {N_H=24=2N_B}}
{
\blank [big]
\externalfigure [assorted-narrow-very-big-error.png] [width=11cm]
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {\m {1/\s} vs \m {\T {\chi}}, assorted, \m {N_H=16=3N_B}}
{
\blank [big]
\externalfigure [assorted-wide-medium-error.png] [width=11cm]
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {\m {1/\s} vs \m {\T {\chi}}, assorted, \m {N_H=24=3N_B}}
{
\blank [big]
\externalfigure [assorted-wide-very-big-error.png] [width=11cm]
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {\It {ibid.}, low signal, \m {N_H=16=3N_B}}
{
\blank [big]
\externalfigure [assorted-medium-low-signal-error.png] [width=11cm]
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {Time, assorted, \m {N_H=16}}
{
\blank [big]
\externalfigure [assorted-medium-time.png] [width=11cm]
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {Discussion}
{
\I DS wins other method in the range drawn.

\I \m {\chi} does not show the trend that \m {\T {\chi}} predicts.

\I The relaxed condition on sparsity may have compromised performance of DS

\I DS has complexity much higher than OMP, but similar to Lasso.

\I As \m {2N_H^2} grows fast, DS becomes less feasible.
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {A Successful Case of Estimation}
{
\blank [big]
\externalfigure [scatter-ddss-success.png] [wfactor=180]

\I Soft thresholding (green) correctly guesses true nonzero components (black), and refines the estimated value (orange)
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {A Failed Case of Estimation}
{
\blank [big]
\externalfigure [scatter-ddss-failure.png] [wfactor=180]

\I Soft thresholding (green) guesses wrong nonzero components (black), and the estimated value (orange) deteriorates
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {Complexity}
{
\I Let \m {C} denote big-O estimation of complexity, then we argue in the treatise
\Disp {
\NC C_{\Rm {OMP}} =\NC \mathcal {O} \SB {N_H^2 \log N_B} \NR
\NC C_{\Rm {Lasso}} =\NC \mathcal {O} \SB {N_H^6} \NR
\NC C_{\Rm {DS}} =\NC \mathcal {O} \SB {N_{H}^6} \NR
}

\I Fitting runtime statistics one has
\Disp {
\NC \T {C}_{\Rm {OMP}} \NC \eqsim \mathcal {O} \SB {N_H^{3.6}} \NR
\NC \T {C}_{\Rm {Lasso}} \NC \eqsim \mathcal {O} \SB {N_H^{4.8}} \NR
\NC \T {C}_{\Rm {DS}} \NC \eqsim \mathcal {O} \SB {N_H^{8.3}} \NR
}
}


