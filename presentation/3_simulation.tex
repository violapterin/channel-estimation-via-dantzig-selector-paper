% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {Linear Programming}
{
\I DS is equivalent to a linear program, which allows more efficient convex programming methods to be used.

\I Calculate
\Disp {
\NC \Hat {\V {g}}, \Hat {\V {f}}
\LA \NC \startcases
   \NC \Min {\V {g}', \V {f}'}  \MC \IP {\V {1}, \V {f}'} \NR
   \NC \Rm {subject} \; \Rm {to} \MC \V {g}' \leq \V {f}' \NR
   \NC \MC - \V {g}' \leq \V {f}' \NR
   \NC \MC \M {P}^\Adj \M {P} \V {g}' \leq \M {P}^\Adj \V {y} + \g_{\Ss {DS}} \V {1} \NR
   \NC \MC - \M {P}^\Adj \M {P} \V {g}' \leq - \M {P}^\Adj \V {y} + \g_{\Ss {DS}} \V {1} \NR
\stopcases \NR
}
\I Output \m {\Hat {\M {H}} \LA \M {K} \Rm {vec}^{-1} \SB {\Hat {g}} \M {K}^\Adj}
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {Performance Metric}
{
\I Let the \m {\T {\chi}}, the relative 2-norm error, be the performance metric.

\Disp {
\NC \T {\chi}
=\NC \RB {
   \F {\log_2 {\VNm {\V {h} -\Hat {\V {h}}} _2}}
   {\log_2 {\VNm {\V {h}}_2}}
} _{\Ss {avg}}, \NR
}

\I Also define \m {\chi} by plugging in \m {\V {d}} into \m {\T {\chi}}.
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {\m {1/\s} vs \m {\T {\chi}}, assorted, \m {N_H=16=2N_Y}}
{
\blank [big]
\externalfigure [assorted-narrow-medium-error.png] [width=11cm]
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {\m {1/\s} vs \m {\T {\chi}}, assorted, \m {N_H=24=2N_Y}}
{
\blank [big]
\externalfigure [assorted-narrow-very-big-error.png] [width=11cm]
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {\m {1/\s} vs \m {\T {\chi}}, assorted, \m {N_H=16=3N_Y}}
{
\blank [big]
\externalfigure [assorted-wide-medium-error.png] [width=11cm]
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {\m {1/\s} vs \m {\T {\chi}}, assorted, \m {N_H=24=3N_Y}}
{
\blank [big]
\externalfigure [assorted-wide-very-big-error.png] [width=11cm]
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {\It {ibid.}, low signal, assorted, \m {N_H=16=3N_Y}}
{
\blank [big]
\externalfigure [assorted-medium-low-signal-error.png] [width=11cm]
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {Time, assorted, \m {N_H=16}}
{
\blank [big]
\externalfigure [assorted-medium-time.png] [width=11cm]
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {Discussion}
{
\I DS wins other method in the range drawn.

\I \m {\chi} does not show trend that \m {\T {\chi}} predicts.

\I The relaxed sparsity might compromise DS performance

\I DS has complexity much higher than OMP, but similar to Lasso.

\I As \m {2N_H^2} grows fast, DS becomes less feasible.
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {A Successful Case of Estimation}
{
\blank [big]
\externalfigure [scatter-ddss-success.png] [wfactor=180]

\I Soft thresholding (green) correctly guesses true nonzero components (black), and refines the estimated value (orange)
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {A Failed Case of Estimation}
{
\blank [big]
\externalfigure [scatter-ddss-failure.png] [wfactor=180]

\I Soft thresholding (green) guesses wrong nonzero components (black), and the estimated value (orange) deteriorates
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {Complexity}
{
\I Let \m {C} denote big-O estimation of complexity, then we argue in the treatise
\Disp {
\NC C_{\Rm {OMP}} =\NC \mathcal {O} \SB {N_H^2 \log N_Y} \NR
\NC C_{\Rm {Lasso}} =\NC \mathcal {O} \SB {N_H^6} \NR
\NC C_{\Rm {DS}} =\NC \mathcal {O} \SB {N_{H}^6} \NR
}

\I Fitting runtime statistics one has
\Disp {
\NC \T {t}_{\Rm {OMP}} \NC \eqsim \mathcal {O} \SB {N_H^{3.6}} \NR
\NC \T {t}_{\Rm {Lasso}} \NC \eqsim \mathcal {O} \SB {N_H^{4.8}} \NR
\NC \T {t}_{\Rm {DS}} \NC \eqsim \mathcal {O} \SB {N_H^{8.3}} \NR
}
}


