% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {Linear Programming}
{
\I DS is equivalent to a linear program, which allows more efficient convex programming methods to be used.

\I Calculate
\Disp {
\NC \Hat {\V {g}}, \Hat {\V {f}}
\LA \NC \startcases
   \NC \Min {\V {g}', \V {f}'}  \MC \IP {\V {1}, \V {f}'} \NR
   \NC \Rm {subject} \; \Rm {to} \MC \V {g}' \leq \V {f}' \NR
   \NC \MC - \V {g}' \leq \V {f}' \NR
   \NC \MC \M {P}^\Adj \M {P} \V {g}' \leq \M {P}^\Adj \V {y} + \g_{\Ss {DS}} \V {1} \NR
   \NC \MC - \M {P}^\Adj \M {P} \V {g}' \leq - \M {P}^\Adj \V {y} + \g_{\Ss {DS}} \V {1} \NR
\stopcases \NR
}
\I Output \m {\Hat {\M {H}} \LA \M {K} \Rm {vec}^{-1} \SB {\Hat {g}} \M {K}^\Adj}
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {Two-Stage Estimation}
{
\I Cand√®s and Tao suggests a two stage version may be used to improve precision.

\I Choose \m {N_0 = 2 N_H^2, N_2 = \lfloor 4 \log N_H \rfloor, N_1 = \lfloor \R {N_0 N_2} \rfloor}

\I Apply DS to \m {\V {y}, \M {P}} to get \m {\Hat {\V {g}}_0}, and take the \m {N_1} largest component of \m {\Hat {\V {g}}_0} to be \m {\V {g}_1}, and corresponding columns of \m {\M {P}} to be \m {\M {P} _1}.

\I Apply DS to \m {\V {y}, \M {P}_1} to get \m {\Hat {\V {g}}_1}, and take the \m {N_2} largest component of \m {\Hat {\V {g}}_1} and to be \m {\V {g}_2}, and corresponding columns of \m {\M {P}} to be \m {\M {P} _2}.

\I Apply least square to \m {\V {y}, \M {P} _2} to get \m {\Hat {\V {g}}_2}, which corresponds to \m {\Hat {\V {g}}}.
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {Performance Metric}
{
\I We call the following quantity \m {\chi} efficiency, which is suggestive of the MIMO channel capacity.

\I Denote \m {\M {E} = \Hat {\M {H}} - \M {H}}.
\Disp {
\NC \chi
= \log \det \SB {\M {I} + \RB {\s + \F {2 \VNm {\M {E}} _2 \VNm {\M {H}} _2} {N_H ^{3/2}}} ^{-1} \M {H} \M {H} ^ \Adj} \NR
}

\I The theoretical efficiency \m {\chi _{\Rm {th}}} is defined by plugging in \m {\V {d}}.
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {Efficiency, \m {N_H=8}}
{
\blank [big]
\externalfigure [assorted-very-small-spectral.png] [factor=fit]
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {Efficiency, \m {N_H=12}}
{
\blank [big]
\externalfigure [assorted-small-spectral.png] [factor=fit]
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {Efficiency, \m {N_H=16}}
{
\blank [big]
\externalfigure [assorted-medium-spectral.png] [factor=fit]
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {Efficiency, \m {N_H=20}}
{
\blank [big]
\externalfigure [assorted-big-spectral.png] [factor=fit]
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {Efficiency, \m {N_H=24}}
{
\blank [big]
\externalfigure [assorted-very-big-spectral.png] [factor=fit]
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {Time, \m {N_H=16}}
{
\blank [big]
\externalfigure [assorted-medium-time.png] [factor=fit]
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {Discussion}
{
\I DS wins narrowly for smaller \m {\s}, but deteriorates for larger \m {\s}.

\I \m {\chi _{\Ss {th}}} is accurate for larger \m {\s}, but overestimates for smaller \m {\s}.

\I The weaker constraint on sparsity conpromises DS's performance.

\I DS has complexity much higher than OMP, but similar to Lasso.

\I As \m {2N_H^2} grows fast, DS becomes less feasible.

\I The two-stage version is not always better.
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {A Successful Case of Estimation}
{
\blank [big]
\externalfigure [scatter-ddss-success.png] [wfactor=150]

\I When DS succeeds: soft thresholding (green) correctly guesses the true values of non-zero positions (black), and refines the estimated value (orange)
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {A Failed Case of Estimation}
{
\blank [big]
\externalfigure [scatter-ddss-failure.png] [wfactor=150]

\I When DS fails: soft thresholding (green) does not correctly guess the true values of non-zero positions (black), and the estimated value (orange) becomes worse
}
% XXX % % XXX % % XXX % % XXX % % XXX % % XXX %
\Frame {Complexity}
{
\I The number of Newton steps is about \m {\MC {O} \SB {\VNm {\V {g} _0 - \V {g} ^\star} _1}}, where \m {\VNm {\V {g} _0} _1} is the starting value, \m {\V {g} ^\star} the true value.
Assume that is \m {\MC {O} \SB {\VNm {\M {P} ^\Adj \V {z}} _1}}, or \m {\MC {O} \SB {N_H^2}}.

\I Every Newton step requires a matrix inversion of dimension \m {2 N_H^2}, with complexity \m {\MC {O} \SB {N_H^4}}, giving overall complexity \m {\MC {O} \SB {N_H^6}}.

\I Complexity of Lasso can be similarly argued to be \m {\MC {O} \SB {N_H^6}}.

\I Clearly, OMP has complexity \m {\MC {O} \SB {N_H^2}}, much smaller than DS.
}


