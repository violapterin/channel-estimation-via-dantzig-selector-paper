\startchapter [title={Introduction}]

\startsection [title={Estimation of MIMO channel}]

Multiple-input multiple-output (MIMO) communication system will be part of the 5G specification \cite [RSM13].
With a large number of antennae on both transmitter and receiver, MIMO is expected to provide a large signal gain.
The appeal of MIMO includes multiplexing gain (by parallel transmission of data), diversity gain (by redundant transmission with space-time coding), and antenna gain (by suitable beamforming to improve signal level).
The millimeter wave (mm-wave) is also proposed, since its smaller wavelength, and thus higher frequency, results in wider bands available.
The antennae, moreover, may be more closer-spaced, making it possible to increase their number.

However, MIMO also gives rise to higher complexity, hence higher hardware overhead and power consumption.
To design new algorithms that address these issues, it is necessary to obtain channel state information, so that channel capacity, among other metrics, may be obtained.
Not only is the channel estimation difficult, due to the large antennae array, but the transmission in higher frequency is subject to noise corruption too.
As a result, conventional training-based algorithms lead to large time and space complexity.

\stopsection

\startsection [title={Compressive sensing}]

Indeed, the estimation of MIMO channel amounts to determining all parameters of the channel, which is a big square matrix of dimension \m {N_H}, the number of antennae.
Fortunately, physical evidence has suggested that mm-wave channel are poor in scattering \cite [ALS14], reducing the number of paths, and that's when a recent development called compressive sensing may help.
Generally speaking, it is a technique for reconstructing a underdetermined linear system, when the sparsity of solution guarantees successful recovery in most cases.

\blank [big]
\externalfigure [compressive-sensing.png] [wfactor=fit, hfactor=fit]
\FigureCaption {
A generic situation in which a signal is “crowded” in the given basis, but is “sparse” in transformed basis.
(Retrieved from \goto {\hyphenatedurl {Wikimedia Commons}} [url (commons.wikimedia.org/wiki/File:Orthogonal_Matching_Pursuit.gif)])
}
\blank [big]

More measurements are required to recover the signal in the former, while fewer suffice in the latter basis.
But, even if we are guaranteed latter basis exists, it is still a nontrivial undertaking to construct it.

Candès and Tao \cite [Can05] first embarked a series of investigation on the problem.
Let us say the number of model parameters \m {N_p \in \MB {N}} is much larger than the number of measurements \m {N_m \in \MB {N}}, that is,
%
\DispNum {N:p:Nm:Nm} {
\NC N_p \gg \NC N_m \NR
}
%
To start, consider \m {\V {x}, \V {x}' \in \MB {K} ^{N_p}} and \m {\M {Q} \in \MB {K} ^{N_m \D  N_p}}, and the linear transformation
%
\DispNum {y:y:Qx:Qx} {
\NC \V {y}
=\NC \M {Q} \V {x} + \V {z} \NR
}
Certainly, \m {\M {Q} \RB {\V {x}_1 - \V {x}_2} = 0}, then \m {\V {x}_1, \V {x}_2} are indistinguishable.
But if \m {\V {x}} is supposed to be sparse, it essentially restricts the system so much that we obtain unique solution for most of the time.

To define this, we say \m {\V {x}} is \m {s}-sparse, if only at most \m {s} components of \m {x} is nonzero.
That is, if there is some \m {\MS {A} \subseteq \CB {0, \dots, N_p-1}} such that
%
\DispNum {x:A:x::x:} {
\NC \V {x} _{\SB {\MS {A}}}
=\NC \V {x} \NR
}
%
with
%
\DispNum {A:A:s::s:} {
\NC \Nm {\MS {A}} \leq \NC s. \NR
}

Candès and Tao showed that, in the noiseless case, a \m {\ell_1}-minimization program recovers the \m {N_m}-dimensional signal, with fewer than \m {N_p} measurement, under overwhelming probability.

As an application, it often occurs that a camera takes a high definition photo, and compresses the image later.
But it may be desirable that a camera equipped with fewer sensors shall take a lower-resolution photo, to save computation and storage, and shall recover the image later.
Such advantage can be useful in medical imaging, since it is not only expensive, but also harmful, to take too many images, while the imaging accuracy may be critical to the diagnosis too \cite [CaT07].

\stopsection

\startsection [title={The Dantzig Selector}]

Now, if the linear transformation is subject to noise \m {z},
%
\DispNum {y:y:Qx:xz} {
\NC \V {y}
=\NC \M {Q} \V {x} + \V {z} \NR
}
is it still possible to recover \m {\V {x}}?

We agree on some conventions.
For \m {\MS {A} \subseteq \CB {0, \dots, N_p-1}}, denote
%
\Disp {
\NC \V {x}  _{\SB {\MS {A}}}
=\NC \sum _{i \in \MS {A}} \V {v} \V {u} _{i} \NR
%
\NC \M {Q}  _{\SB {\MS {A}}}
=\NC \sum _{i \in \MS {A}} \M {Q} _{\SB {:,i}} \NR
}
%
That is, respectively, the components of \m {\V {x}}, and the columns of \m {\M {A}}, that have indices in \m {\MS {A}}.


\Result
{Definition}
{
For fixed \m {s =0, \dots N_p -1}, we say that \m {\M {Q}} satisfies \m {\d_s}-restricted isometry property (hereafter \m {\d_s}-RIP) of sparsity \m {s} with respect to \m {0 \leq \d_s \leq 1}, if for all \m {s}-sparse \m {\V {x}}
%
\DispNum {1:2:Qx:22} {
\NC \RB {1-\d_s} \VNm {\V {x}} _2 ^2
\leq \NC \VNm {\M {Q} \V {x}} _2 ^2 \NR
%
\NC \leq \NC \RB {1+\d_s} \VNm {\V {x}} _2 ^2 \NR
}
}
%
It helps to think that \m {\M {Q}} is almost unitary up to relative error \m {\d_s}.

For concreteness, say \m {\V {z}} is an i.i.d.\ normalized random normal vector.
If so, a stronger result was established \cite [CaT07] that, with another \m {\ell_1} minimization program called Dantzig Selector (hereafter DS), recovery under the noisy case is again possible.

\Result
{Algorithm}
{
\startitemize[n]
\item Input \m {\M {Q} \in \MB {R} ^{N_m \D N_p}} and \m {\V {y} \in \MB {R} ^{N_m}}.
%
\item Calculate
%
\DispNum {h:h:mi:yg'} {
\NC \Hat {\V {h}}
\LA \NC \startcases
\NC \Min {\V {h}'} \MC \VNm {\V {h}'} _1 \NR
%
\NC \Rm {subject} \; \Rm {to} \Q \MC \VNm {\M {Q}^\Adj \RB {\M {Q} \V {h}' -\V {y}}} _\infty \leq \g \NR
\stopcases \NR
}
\item Output \m {\Hat {\V {h}}}.
\stopitemize
}

Particularly, Algorithm [2] recovers \m {\V {x}} with the expected square error bounded with overwhelming probability.

From the computational perspective, the formulation as an \m {\ell_1} minimization problem, which is convex, allows techniques from convex optimization to be used.
In fact, Candès \& Romberg provided an example implementation on their website (Candès \& Romberg \cite [CaR05]).
In addition, the \m {\ell_1}-minimization problem with \m {\ell_\infty}-constraint may be recast as a Linear Program (hereafter LP), rendering convex programming technique applicable.

Here the constant \m {\d_s} in Definition [1] is crucial:
we must make a series of carefully chosen but random measurement.
We can't just take \m {\M {Q}} to be any unitary matrix, for which it is trivial that \m {\d_s =0}.
One common proposal is choosing i.i.d.\ entries of \m {\M {Q}}, and it was established by Baraniuk et.\ al.\ \cite [BDD08] that if \m {\VNm {\M {Q} \V {x}}} concentrates sharply, \m {\M {Q}} has RIP for overwhelming probability, but it remains to conceive a algorithm that efficiently finds RIP matrices.

Back at the ranch, recall that the channel estimation has been considered infeasible in the MIMO setting, but luckily, physical evidences suggest that mm-wave channels are in fact sparse in the frequency domain.
Some scholars have thus applied compressive sensing techniques.
An early attempt was Bajwa et.\ al.\ \cite [BHS10], where they argued that 
if the \m {\ell_0}-norm of the channel matrix may be bounded by a constant, DS may be applied to estimate the time-dependent single-antenna channel response.
The accompanying note by the same group \cite [BHR08] showed that \m {X} is RIP for overwhelming probability, justifying their work.
And \cite [BHR08], which considered a linear time-invariant channel, showed that if the convolution with channel response can be seen as a linear system, whose sensing matrix has RIP of overwhelmingly probability, and DS is readily applied.

\stopsection

\startsection [title={Orthogonal Matching Pursuit}]

At the same time, Tropp and Gilbert \cite [TrG07b] suggested that a greedy algorithm called Orthogonal Matching Pursuit (OMP), which also makes use of an i.i.d.\ random matrix, may also reconstruct the sparse signal sufficiently well in an overwhelmingly probability. 
In so doing, we still consider \Rf {y:y:Qx:xz}, and pick up the columns of \m {\M {Q}} greedily, hoping to correspond to the nonzero components of \m {\V {x}}.

\Result
{Algorithm}
{
\startitemize[n]
\item Input \m {\M {Q} \in \MB {R} ^{N_m, N_p}} and \m {\V {y} \in \MB {R} ^{N_m}}, \m {\eta >0}.
%
\item Initialize
%
\Disp {
\NC \V {r}
\LA \NC \V {y} \NR
%
\NC S
\LA \NC \varnothing \NR
}
\item Start the loop with counter \m {i \LA 1}.
\item Find
%
\DispNum {c:c:i0:ir} {
\NC c
\LA \NC \underset {i =0, \dots, N_p-1} {\Rm {argmax}}
\Nm {\M {Q} _{\SB {:,i}} \V {r}} \NR
}
%
and insert \m {S \LA S \cup {i}}.
\item Compute
%
\DispNum {Q:Q:QS:Qy} {
\NC \M {Q} ^\ddagger
\LA \NC \RB {\M {Q} _{\SB {S}} ^\Adj \M {Q} _{\SB {S}}} ^{-1} \M {Q} _{\SB {S}} ^\Adj \NR
%
\NC \V {r}
\LA \NC \V {y} -\M {Q} _{\SB {S}} ^\Adj \M {Q} ^\ddagger \V {y} \NR
}
\item Break if
%
\DispNum {r:2:h':h'} {
\NC \VNm {\V {r}} _2
<\NC \eta \NR
}
%
where \m {\VNm {\M {Q} ^\ddagger} _1} denotes the operator norm.
\item Output 
%
\DispNum {g:g:Qy:Qy} {
\NC \Hat {\V {g}}
\LA \NC \M {Q} ^\ddagger \V {y} \NR
}
\stopitemize
}
%
Here \m {\V {r}} can be thought of as the estimated noise, and \m {S} the number of estimated nonzero components of \m {\V {x}} (here we have slightly abused the usage of subscript).
From Tropp and Gilbert \cite [TrG07a], the probability that OMP recovers \m {\V {x}} completely is overwhelming, providing a performance guarantee.
It is instructive to note that the matrix consisting of a collection of columns that likely spans \m {\V {x}} plays the role of the sensing matrix in the previous section.

OMP has since been applied widely in the literature.
Alkhateeb et.\ al.\ \cite [AEL14] proposed an adaptive algorithm with a beam codebook for quantized angles of arrival and departure.
Alkhateeb, Leus, and Heath Jr.\ \cite [ALH15] discussed the trade-off between number of OMP measurement and accuracy in OMP, applied on an all-phase-shifter beamformers, with quantized, non-uniform, predetermined angles.
Gao et.\ al.\ \cite [LGC16] proposed a jointly reconstruction, with a modified OMP, of several high-dimensional sparse signals with restriction on nonzero components.
Hu, Wang, and He \cite [HWH13] applied OMP to estimate quantized path delay of OFDM subcarriers, assuming the channel gain in each path is stationary.
Lee, Gil, and Lee \cite [LGL16] considered a hybrid system, with the effective beamformer serving as sensing matrix, where set of quantized angles was specially designed, and permuted DFT matrix is used.

Some further attempts include Manoj and Kannu \cite [MaK17], who formulated the sparse condition on submatrices of the signal for successful reconstruction.
Gurbuz, Yapici, and Guvenc \cite [GYG18] introduced perturbation of the quantized set of angles, on which the sensing matrix is based.
Panayirci et.\ al.\ \cite [PAU19] suggested, in addition to application of OMP, a concatenation of max-likelihood estimation of model parameters, and maximum-a-posteriori estimation of channel gains.

Performance guarantee of OMP is also studied on various conditions.
Cai, Wang, and Xu \cite [CWX10], under assumption of low coherence of columns of matrices, instead of almost-unitarity, gave new bound on OMP.
Cai and Wang \cite [CaW11] extended the study to DS and other convex programs for sparse recovery, and Ben-Haim et.\ al.\ \cite [BEE10] followed up and refined their bounds, concluding that OMP is better for low-SNR scenario, and DS is better for high-SNR.

\stopsection

\startsection [title={Contribution}]

We saw that the literature on channel estimation seems to favored OMP for its low complexity.
Then one naturally wonders how greedy algorithms, like OMP, compare to convex programs, like DS.
Can it be that greedy algorithms trade precision for complexity, and that the opposite is true of convex programs?

We consider a hybrid system with uniform linear array as in Lee, Gil, and Lee \cite [LGL16], and aim to compare their result.
Their work proposed an optimal beamformer, jointly designed on both ends via a nonconvex optimization problem, and due to its difficulty they have made considerable approximation.
Is the simplified beamformer still optimal for OMP?
Or will there be mismatch between the angles of sensing matrix and their true values, compromising the algorithm's accuracy?

In this treatise, we illustrate, by the case of uniformly random angles, that DS may outperform OMP among other methods, and the nature of convex program also guarantees its performance.
In chapter 2 we shall see that the effective beamformer may serve as the sensing matrix having RIP for high probability.
The sparsity can be exploited this way, and DS may readily be applied.
In chapter 3, along the lines of Candès and Tao \cite [CaT07], we give a bound (which holds for high probability) on expected error norm, expressed in terms of the suggested sparsity and the number of paths.
In chapter 4, we remark that DS can be cast as a linear program, for which efficient algorithms are known.
Numerical results show that in our configuration DS is superior to OMP among other methods, but is achieved at the cost of higher complexity.


\stopsection

\stopchapter

