\startchapter [title={Simulation}]

In this chapter we shall verify the bound \m {\T {\chi}} by numerical experiments.
DS is a convex optimization program, and for present purpose, we choose the Python library CVXPY.
For the implementation details, see Diamond and Boyd \cite [DiB16]

A direct implementation of Algorithm [7] exhibits extraordinarily large complexity, but Candès and Romberg \cite [CaR05] showed that DS can be simplified as an linear program (LP) (even though they assumed real vectors, and we consider the complex ones), which we reproduce below.

\startsection [title={Method}]

\startsubsection [title={Some Notation}]

For brevity we shall represent complex vector and matrices by real ones, comprising of its real an imaginary parts.
For \m {\V {x} \in \MB {C} ^{M}}, define the real representation \m {\MS {R} \SB {\V {x}}} of \m {\V {x}} to be
%
\DispNum {R:x:R2:M1} {
\NC \MS {R} \SB {\V {x}}
\in \NC \MB {R} ^{2M} \NR
\NC \MS {R} \SB {\V {x}} _{\SB {m}}
= \NC \startcases
\NC \MF {Re} \SB {\V {x} _{\SB {m'}}}, \MC m =2m' \NR
\NC \MF {Im} \SB {\V {x} _{\SB {m'}}}, \MC m =2m'+1 \NR
\stopcases \NR
\NC m' 
= \NC  0, 1, 2, \ldots, M-1 \NR
}
%
The injection is obvious, and we may define \m {\MS {R} ^{-1}} so that
%
\DispNum {R:1:x::x:} {
\NC \MS {R} ^{-1} \SB {\MS {R} \SB {\V {x}}}
=\NC \V {x} \NR
}
%
Accordingly, the following generalization to complex matrices is valid, once we call the ring representation of complex numbers.
For \m {\M {A} \in \MB {C} ^{M_1 \D M_2}}, define real representation \m {\MS {R} \SB {\M {A}}} of \m {\M {A}} to be
%
\DispNum {R:A:R2:21} {
\NC \MS {R} \SB {\M {A}}
\in \NC \MB {R} ^{2M_1 \D 2M_2} \NR
\NC \MS {R} \SB {\M {A}} _{\SB {m_1,m_2}} =
\NC \startcases
\NC \MF {Re} \SB {\M {A} _{\SB {m_1',m_2'}}}, \MC (m_1, m_2) = (2m_1', 2m_2') \NR
\NC \MF {Im} \SB {\M {A} _{\SB {m_1',m_2'}}}, \MC (m_1, m_2) = (2m_1'+1, 2m_2') \NR
\NC -\MF {Im} \SB {\M {A} _{\SB {m_1',m_2'}}}, \MC (m_1, m_2) = (2m_1', 2m_2'+1) \NR
\NC \MF {Re} \SB {\M {A} _{\SB {m_1',m_2'}}}, \MC (m_1, m_2) = (2m_1'+1, 2m_2'+1) \NR
\stopcases \NR
\NC m_1 
= \NC 0, 1, 2, \ldots, M_1 -1 \NR
\NC m_2 
= \NC 0, 1, 2, \ldots, M_2 -1 \NR
}

With these, we define
%
\DispNum {y:::Ry:Ny} {
\NC \T {\V {y}}
= \NC \MS {R} \SB {\V {y}}
\in \MB {R} ^{2 N_Y^2} \NR
%
\NC \T {\V {g}}
= \NC \MS {R} \SB {\V {g}}
\in \MB {R} ^{2 N_H^2} \NR
%
\NC \T {\M {P}}
= \NC \MS {R} \SB {\M {P}}
\in \MB {R} ^{2 N_Y^2 \D 2 N_H^2} \NR
%
\NC \T {\M {P}} ^\Adj
= \NC \MS {R} \SB {\M {P} ^\Adj}
\in \MB {R} ^{2 N_Y^2 \D 2 N_H^2} \NR
%
\NC \T {\V {z}}
= \NC \MS {R} \SB {\V {z}}
\in \MB {R} ^{2 N_Y^2} \NR
}
%
such that, by construction,
%
\DispNum {y:::Pg:gz} {
\NC \V {\T {y}}
= \NC \M {\T {P}} \V {\T {g}} +\V {\T {z}} \NR
}

Furthermore, introduce the following matrices to identify the components where we want to take \m {\ell_2}-norm, in the manner similar to an indicator function.
%
\DispNum {u:h:RN:h1} {
\NC {\V {u}}_{n_h}
\in \NC \MB {R} ^{N_H^2} \NR
\NC \RB {\V {u}_{n_h}} _{\SB {n_h'}}
= \NC
\startcases
1, \Q \MC n_h' =n_h \NR
0, \Q \NC \Rm {otherwise} \NR
\stopcases \NR
\NC \T {\M {U}}_{n_h} \in \NC \MB {R} ^{2 N_H^2 \D 2 N_H^2} \NR
\NC \RB {\T {\M {U}}_{n_h}} _{\SB {n_h', n_h''}}
= \NC
\startcases
1, \Q \MC n_h' =2n_h,\; n_h'' =2n_h \NR
1, \Q \MC n_h' =2n_h +1,\; n_h'' =2n_h +1 \NR
0, \Q \NC \Rm {otherwise} \NR
\stopcases \NR
\NC n_h, n_h', n_h'' 
= \NC 0, 1, 2, \ldots, N_H^2 -1 \NR
}
%
And denote \m {\V {1}} to be the all-\m{1} vector for short, and \m {\V {0}} the all-\m{0} vector, whose dimension will be specified from context.
Now, if we introduce vector \m{\V {m}}
%
\DispNum {m:::RN:h1} {
\NC \V {m} \in \NC \MB {R} ^{N_H^2} \NR
\NC \V {m} _{\SB{n_h}}
= \NC \Nm{\V {g} _{\SB {n_h}}} \NR
\NC n_h, n_h', n_h'' 
= \NC 0, 1, 2, \ldots, N_H^2 -1 \NR
}
%
to denote the entrywise complex modulus of \m{\V {g}}, we see that the convex optimization now takes the form
%
\DispNum {g:::mi:h1} {
\NC \Hat {\T {\V {g}}}
= \NC \startcases
\NC \Min {\T {\V {g}}', \V {m}} \Q
\MC \IP { \V {1}, \V {m} } \NR
\NC \Rm {subject} \; \Rm {to} \Q
\MC \VNm { \T {\M {U}}_{n_h} \T {\V {g}}' }_2
\leq \IP { \V {u}_{n_h}, \V {m} } \NR
\NC \MC \VNm { \T {\M {U}}_{n_h} \T {\M {P}}^\Adj \RB { \T {\M {P}} \T {\V {g}}' -\T {\V {y}} } }_2
\leq \g_{\Rm {DS}} \NR
\stopcases \NR
\NC n_h 
= \NC 0, 1, 2, \ldots, N_H^2-1 \NR
}

\stopsubsection

\startsubsection [title={A Linear Program}]

We shall see furthermore that the program can be cast into an SOCP.
To make the point clearer, we rewrite the program just above into an extended block matrix form, by defining auxiliary variables as follows,
%
\DispNum {t:t:01:Nh} {
\NC \V {t}
= \NC \startTheMatrix
\NC \V {0} \NR
\NC \V {1} \NR
\stopTheMatrix
\in \MB {R} ^{3 N_H^2} \NR
%
\NC \V {x}'
= \NC \startTheMatrix
\NC \T {\V {g}}' \NR
\NC \V {m} \NR
\stopTheMatrix
\in \MB {R} ^{3 N_H^2} \NR
}
%
with
%
\DispNum {A:i:Ui:21} {
\NC \M {A}_i
= \NC \startTheMatrix
\NC \T {\M {U}}_{i}, \NC \M {0} \NR
\stopTheMatrix
\in \MB {R} ^{2 N_H^2 \D 3 N_H^2} \NR
%
\NC \V {b}_i
= \NC \V {0}
\in \MB {R} ^{2 N_H^2} \NR
%
\NC \V {c}_i
= \NC \startTheMatrix
\NC \V {0} \NR
\NC \V {u}_{i} \NR
\stopTheMatrix
\in \MB {R} ^{3 N_H^2} \NR
%
\NC d_i
= \NC 0 \NR
%
\NC i 
= \NC 0, 1, 2, \ldots, N_H^2 -1 \NR
}
%
and
%
\DispNum {A:i:Ui:21:"} {
\NC \M {A}_i
= \NC \startTheMatrix
\NC -\T {\M {U}}_{i - N_H^2} \T {\M {P}}^\Adj \T {\M {P}}, \NC \M {0} \NR
\stopTheMatrix
\in \MB {R} ^{2 N_H^2 \D 3 N_H^2} \NR
%
\NC \V {b}_i
=\NC \T {\M {U}}_{i - N_H^2} \T {\M {P}}^\Adj \T {\V {y}}
\in \MB {R} ^{2 N_H^2} \NR
%
\NC \V {c}_i
= \NC \V {0}
\in \MB {R} ^{3 N_H^2} \NR
%
\NC d_i
= \NC \g_{\Rm {DS}} \NR
%
\NC i 
= \NC N_H^2, N_H^2 +1, N_H^2 +2, \ldots, 2 N_H^2 -1 \NR
}

\Result
{Algorithm}
{
\startitemize[n]
\item Input \m{\M {P} \in \MB {C} ^{ N_Y^2 \D N_H^2}}, \m{\V {y} \in \MB {C} ^{ N_Y^2}}, \m {\g_{\Rm {DS}} > 0}.
\item Define \m{\V {t}, \V {x}, \M {A}_i, \V {b}_i, \V {c}_i, d_i} according to \Rf {t:t:01:Nh}, \Rf {A:i:Ui:21}, \Rf {A:i:Ui:21:"}.

\item Calculate
%
\DispNum {x:::mi:h1} {
\NC \Hat {\V {x}}
\LA \NC \startcases
\NC \Min {\V {x}' \in \MB {C} ^{N_H^2}}
\MC \IP {\V {t}, \V {x}'} \NR
\NC \Rm {subject} \; \Rm {to}
\Q  \MC \VNm {\M {A} _{i} \V {x}' +\V {b} _{i}} _2
\leq \IP {\V {c}_{i}, \V {x}'} +\V {d}_i \NR
\NC \MC i 
=0, 1, 2, \ldots, 2 N_H^2 -1 \NR
\stopcases \NR
}
\item Extract
%
\DispNum {g:::x0:h1} {
\NC \Hat{\T{\V {g}}}
\LA \NC \Hat{\V {x}} _{\SB {0 : 2 N_H^2-1}} \NR
}
\item Convert 
%
\DispNum {g:::R1:1g} {
\NC \Hat{\V {g}}
\LA \NC \MS {R} ^{-1} \SB {\Hat{\T{\V {g}}}} \NR
}
\item Calculate
%
\DispNum {G:G:ve:1g} {
\NC \Hat {G}
\LA \NC \Rm {vec}^{-1} \SB {\Hat {g}} \NR
}
\item Calculate
%
\DispNum {H:H:KG:GK} {
\NC \Hat {\M {H}}
\LA \NC \M {K} \Hat {\M {G}} \M {K}^\Adj \NR
}
\item Output \m {\Hat {\M {H}}}.
\stopitemize
}

\stopsubsection

\startsubsection [title={A Two-Stage Version}]

We also tried another version of DS, in which DS is applied twice in estimated nonzero components.
The two-stage method was briefly mentioned \cite [CaT07] but without further justification by numerical experiments.
To apply the method, we first apply DS as before.
Then, we extract largest components of the estimated vector, and apply DS again.
Furthermore, we extract largest components of the second estimated vector, and apply LS.
The final vector's indices has to be scrambled back according to the original index.

\Result
{Algorithm}
{
\startitemize[n]
\item Let \m {\g_{\Rm {DS}} \geq 0} be given, and \m {\M {P}}

\item Set
\Disp {
\NC N_0
=\NC 2 N_H^2, \NR
\NC N_2
=\NC \lfloor 4 \log N_H \rfloor, \NR
\NC N_1
=\NC \lfloor \R {N_0 N_2} \rfloor \NR
}

\item Apply DS to \m {\V {y}, \M {P}} to get \m {\Hat {\V {g}}_0}, call the \m {N_1} largest component of \m {\Hat {\V {g}}_0} and to be \m {\V {g}_1}, and corresponding columns of \m {\M {P}} to be \m {\M {P} _1}.

\item Apply DS to \m {\V {y}, \M {P}_1} to get \m {\Hat {\V {g}}_1}, call the \m {N_2} largest component of \m {\Hat {\V {g}}_1} and to be \m {\V {g}_2}, and corresponding columns of \m {\M {P}} to be \m {\M {P} _2}.

\item Apply least square to \m {\V {y}, \M {P} _2} to get \m {\Hat {\V {g}}_2}, which corresponds to \m {\Hat {\V {g}}}.
\stopitemize
}

Unfortunately, the two-stage method does not always appear to be better than the one-stage method, and we did not include the result in the figures.
It might have something to do with appropriately scaling \m {\g_{\Rm {DS}}} with respect to \m {N_H},
and we do not know what most reasonable choice of \m {N_1, N_1, N_2} is.

\stopsubsection

\startsection [title={Result}]

\startsubsection [title={Settings}]

To get some idea of the order of magnitude of noise-to-signal level, plug in some actual numbers.
Consider only path loss in the simplest form according to Friis Law.
Suppose the power of mobile phone antenna is 0.25 W,
the carrier frequency is 5GHz,
the base station is \m {1.5} km away,
the noise is \m {–40} dB W.
If so, the noise-to-signal ratio is
\DispNum {1:8:02:24} {
\NC 10^{-4} \F {1} {0.25} \R {\F {5 \D 10^9 \D 4 \pi \D 1500} {3 \D 10^8}}
=\NC 0.224 \NR
}
In the following simulation, we use dimensionless noise-to-signal level \m {\s}.
Its value starts with \m {2^{-4}}, and is increased by being multiplied with 2.
For the plot of assorted methods, 8 values of \m {\s} are used; for the plots of only OMP, 9 values are used; for the plots of only OMP, 6 values are used.

We take \m {N_H} to be \m {8, 12, 16, 20, 24}, respectively.
For assorted plots, two series of plots are simulated.
One series for \m {N_Y = \lfloor N_H /2 \rfloor}, giving \m {4, 6, 8, 10, 12};
one series for \m {N_Y = \lfloor N_H /3 \rfloor}, giving \m {3, 4, 5, 7, 8}.
For OMP and DS only plots, we just choose \m {N_Y = \lfloor N_H /3 \rfloor}.
On the other hand, \m {N_R} always set to be \m {\lfloor \RB {\log N_H}^2 \rfloor}.
Unfortunately, this is very far from achieving the design values \Rf {N:B:4l:H2}, and this may be part of the reason the result is not as successful as expected.

Other parameters are fixed in these experiments; 
The number of grid of quantization of analog precoder is \m {16}.
The number of paths \m {L} is \m {4}.
The wavelength of carrier \m {\l _{\Rm {ant}}} is set to be \m {0.1}.
and the antenna spacing \m {d _{\Rm {ant}}} is 0.2.
Only their ratio matters.

For parameters related to the convex optimization algorithm, the tolerance of absolute error in the primal-dual algorithm used in CVXPY is set as \m {10^{-4}}, and the tolerance of relative error, as \m {10^{-3}}.
The maximal number of iteration of CVXPY is set to be \m {32}.
On the other hand, the maximal number of iteration of OMP is set to be \m {4 N_Y}.

Denote the threshold used in DS program to be \m {\g_{\Rm {DS}}}, and similar threshold of Lasso to be \m {\g_{\Rm {Lasso}}}.
Different values are tried by multiplying or dividing them by powers of 2.

In the plots of assorted methods, we set \m {\g_{\Rm {DS}} = 2 \R {\log N_H}} for concreteness, as suggested in \cite [CaT07].
For sake of comparison, \m {\g_{\Rm {Lasso}} = 2 \R {\log N_H}} too.
For OMP, suggested values in Cai \& Wang (2011) is illuminating, where they (in their Theorem 7) considered \m {\ell _2}-norm, and (in their Theorem 8), \m {\ell _\infty}-norm.
We make simplification and set respectively \m {\R {3 N_Y}} and \m {2 \R {\log N_H}}.
Meanwhile, the threshold used for the remainder norm used in OMP program is suggested in \cite [CaW11].
We take \m {\h_{\Rm {OMP}} = 2 \R {\log N_H}} for \m {\infty}-norm constraint, \m {\h_{\Rm {OMP}} = \R {3 N_Y}} for 2-norm constraint.

In the plots of DS only or OMP only, we vary the values of \m {\g_{\Rm {DS}}} and \m {\h_{\Rm {OMP}}} to observe the effects of thresholds.

Each data point for DS and Lasso is repeated for 256 times, and taken average.
Other methods are repeated for more times: OMP for \m {4 \D 256} times, LS for \m {12 \D 256} times.

For performance metric, we follow Lee, Gil, and Lee \cite [LGL16],
\DispNum {h:h:lo:vg} {
\NC \T {\chi}
=\NC \RB {
   \F {\log_2 {\VNm {\V {h} -\Hat {\V {h}}} _2}}
   {\log_2 {\VNm {\V {h}}_2}}
} _{\Ss {avg}}, \NR
}
However, we note that when \m {\VNm {\V {h}}_2} is small, this can blow up.
We do not know, either, whether \m {\T {\chi}} is a good indicator of channel capacity, since we do not fix a channel model in our investigation.

The parameters related to precision of the Newton step may be adjusted from CVXPY's class methods.
We set the maximum absolute tolerance to be \m {5 \D 10 ^ {-7}},
the maximum absolute tolerance to be \m {5 \D 10 ^ {-6}},
the maximum feasible tolerance to be \m {5 \D 10 ^ {-7}}.
It remains to be tested what values are most suitable for our purposes.
If the tolerance parameters are too small, the program often gives overflowing values, perhaps because it cannot find feasible solutions.
On the other hand, if they are too big, the steps are so imprecise that the program may fail to solve.

\stopsubsection

\startsubsection [title={Assorted Plots}]

In the following, we plot the reciprocal of noise level vs relative error norm, for \m {N_H = 8, 12, 16, 20, 24}, respectively.
Figures 3 to 6 have \m {N_H = 3 N_Y}, and figures 7 to 10 has \m {N_H = 2 N_Y}.

\blank [big]
\externalfigure [assorted-wide-small-error.png] [wfactor=fit, hfactor=fit]
\FigureCaption {Assorted methods, \m {N_H = 3 N_Y, N_H = 12}, error.}
\blank [big]
%
\blank [big]
\externalfigure [assorted-wide-medium-error.png] [wfactor=fit, hfactor=fit]
\FigureCaption {Assorted methods, \m {N_H = 3 N_Y, N_H = 16}, error.}
%
\blank [big]
\externalfigure [assorted-wide-big-error.png] [wfactor=fit, hfactor=fit]
\FigureCaption {Assorted methods, \m {N_H = 3 N_Y, N_H = 20}, error.}
%
\blank [big]
\externalfigure [assorted-wide-very-big-error.png] [wfactor=fit, hfactor=fit]
\FigureCaption {Assorted methods, \m {N_H = 3 N_Y, N_H = 24}, error.}
%
\blank [big]
\externalfigure [assorted-narrow-small-error.png] [wfactor=fit, hfactor=fit]
\FigureCaption {Assorted methods, \m {N_H = 2 N_Y, N_H = 12}, error.}
\blank [big]
%
\blank [big]
\externalfigure [assorted-narrow-medium-error.png] [wfactor=fit, hfactor=fit]
\FigureCaption {Assorted methods, \m {N_H = 2 N_Y, N_H = 16}, error.}
\blank [big]
%
\blank [big]
\externalfigure [assorted-narrow-big-error.png] [wfactor=fit, hfactor=fit]
\FigureCaption {Assorted methods, \m {N_H = 2 N_Y, N_H = 20}, error.}
\blank [big]
%
\blank [big]
\externalfigure [assorted-narrow-very-big-error.png] [wfactor=fit, hfactor=fit]
\FigureCaption {Assorted methods, \m {N_H = 2 N_Y, N_H = 24}, error.}
\blank [big]

As for the plot of time taken in minutes, we give only one example of \m {N_H = 16}.
%
\blank [big]
\externalfigure [assorted-medium-time.png] [wfactor=fit, hfactor=fit]
\FigureCaption {Assorted methods, \m {N_H = 3 N_Y, N_H = 16}, time.}
\blank [big]

We give one additional plot for extreme low signal level, in order to observe the trend of curves.
\blank [big]
\externalfigure [assorted-medium-low-signal-error.png] [wfactor=fit, hfactor=fit]
\FigureCaption {Assorted methods, \m {N_H = 3 N_Y, N_H = 16}, low signal level, error.}
\blank [big]


\stopsubsection

\startsubsection [title={Plots of DS only}]

Again, we plot the reciprocal of noise level vs relative error norm, for \m {N_H = 8, 12, 16, 20, 24}, respectively, and \m {N_H = 3 N_Y} for each of them.
Here, \m {\T {\chi}} is shown too.
%
\blank [big]
\externalfigure [ddss-small-error.png] [wfactor=fit, hfactor=fit]
\FigureCaption {DS, \m {N_H = 3 N_Y, N_H = 12}, error.}
\blank [big]
%
\blank [big]
\externalfigure [ddss-medium-error.png] [wfactor=fit, hfactor=fit]
\FigureCaption {DS, \m {N_H = 3 N_Y, N_H = 16}, error.}
\blank [big]
%
\blank [big]
\externalfigure [ddss-big-error.png] [wfactor=fit, hfactor=fit]
\FigureCaption {DS, \m {N_H = 3 N_Y, N_H = 20}, error.}
\blank [big]
%
\blank [big]
\externalfigure [ddss-very-big-error.png] [wfactor=fit, hfactor=fit]
\FigureCaption {DS, \m {N_H = 3 N_Y, N_H = 24}, error.}
\blank [big]
%
As for the plot of time taken in minutes, we give only one example of \m {N_H = 16}.
%
\blank [big]
\externalfigure [ddss-medium-time.png] [wfactor=fit, hfactor=fit]
\FigureCaption {DS, \m {N_H = 3 N_Y, N_H = 16}, time.}
\blank [big]

\stopsubsection

\startsubsection [title={Plots of OMP only}]

Again, we plot the reciprocal of noise level vs relative error norm, for \m {N_H = 8, 12, 16, 20, 24}, respectively, and \m {N_H = 3 N_Y} for each of them.

%
\blank [big]
\externalfigure [oommpp-small-error.png] [wfactor=fit, hfactor=fit]
\FigureCaption {OMP, \m {N_H = 3 N_Y, N_H = 12}, error.}
\blank [big]
%
\blank [big]
\externalfigure [oommpp-medium-error.png] [wfactor=fit, hfactor=fit]
\FigureCaption {OMP, \m {N_H = 3 N_Y, N_H = 16}, error.}
\blank [big]
%
\blank [big]
\externalfigure [oommpp-big-error.png] [wfactor=fit, hfactor=fit]
\FigureCaption {OMP, \m {N_H = 3 N_Y, N_H = 20}, error.}
\blank [big]
%
\blank [big]
\externalfigure [oommpp-very-big-error.png] [wfactor=fit, hfactor=fit]
\FigureCaption {OMP, \m {N_H = 3 N_Y, N_H = 24}, error.}
\blank [big]
%
As for the plot of time taken in minutes, we give only one example of \m {N_H = 16}.
%
\blank [big]
\externalfigure [oommpp-medium-time.png] [wfactor=fit, hfactor=fit]
\FigureCaption {OMP, \m {N_H = 3 N_Y, N_H = 16}, time.}
\blank [big]

\stopsubsection

\startsubsection [title={Discussion}]

In the assorted plots, DS steadily outperforms other methods.
From lowest to highest error, it usually goes like this: DS, LS, Lasso, OMP.
Though the result is encouraging for our purpose, we wonder why OMP is so bad.
Did we make mistakes?
We do not think so.
A crucial reason might be that in Lee, Gil, and Lee \cite [LGL16], the angles of departure and arrival is distributed within a cluster of standard deviation 15 degrees, and in ours, they are uniform on the unit circle.
One is then tempted to say, we suggested a low complexity beamformer and high complexity estimation method, while they suggested a high complexity beamformer and low complexity estimation method, and the low complexity solution does not seem effective in our settings.
It remains to see, however, how OMP and DS compare, if done exactly under their settings.

However, it is not clear how higher \m {\s} (dimensionless noise level) corresponds to lower \m {\T {\chi}} (simulated error).
It seems when \m {\s} is further increased (leftmost of the figures), the error might also increase rapidly, but more parameters have to be simulated for a conclusion.

Varying thresholds of DS (namely \m {\g_{\Rm {DS}}}) and OMP (namely \m {\h_{\Rm {OMP}}}) appears not to affect the estimation significantly.
Nevertheless, it seems \m {\g_{\Rm {DS}}} is usually best chosen in the order of magnitude of \m {\s}, which is reasonable.
Sometimes the iteration stabilizes much earlier than is constrained by maximum iteration number.

Sadly, we remark that CVXPY sometimes gives overflowing outputs, some of them as large as \m {10^11}.
This indicates perhaps some typical-looking output may in fact be unreliable.
To rid overflowing values, we discard outputs larger than a given threshold, say \m {10^4}, and simply apply a least square.
It is open whether there are better methods.

We are not sure either, unfortunately, why \m {\T {\chi}} does not agree \m {\chi} (the error bound), and why \m {\chi} does not appear to increase significantly as \m {\s} increases.
As shown above, the \m {\chi} is very constant around 1, thus \m {\T {\chi}} is an overestimation when \m {\s} is high and underestimation when \m {\s} is low.
A possible explanation is that the non-sparsity of \m {\V {g}} undermines the analysis in chapter 3, despite our attempts.
These matters remain to be investigated in the future.

To give some idea of the case that DS works or fails, figures 23 and 24 illustrate either cases.
In both figures the true values of \m {\M {H}} entries are ordered by magnitude, and corresponding estimated values in the first and second stage are plotted against them.
When DS fails, the soft thresholding did not rule out smaller entries, thus a second application of DS on the wrong index set failed too.
When DS succeeded, the soft thresholding gave a reasonable guess of the nonzero components, thus a second application of DS on this index set refined the estimated values.

\blank [big]
\externalfigure [scatter-ddss-failure.png] [wfactor=300]
\FigureCaption {A scatter plot of position versus magnitude for a case in which DS failed.}
\blank [big]

\blank [big]
\externalfigure [scatter-ddss-success.png] [wfactor=300]
\FigureCaption {A scatter plot of position versus magnitude for a case in which DS succeeded.}
\blank [big]

\stopsection

\startsection [title={Complexity}]

\startsubsection [title={Asymtotic Analysis}]

We want to know the complexity of DS and OMP.
In general, complexity of convex programs is difficult to analyze, in that it is not easily to determine how many iterations were done.
Boyd and Vandenberghe \cite [BoV04] analyzed complexity of inequality constrained Newton method, in which self-concordance is assumed (p.496ff, also p.531).
In our case, Candès and Romberg \cite [CaR05] pointed out that the complex DS can be cast into an second order cone programming (SOCP), and where self concordance holds.
Diamond and Boyd (writers of CVXPY) \cite [DiB16] explained that convex programs are usually converted into primal-dual problems.
The library we called in the simulation is ECOS, where Newton method is used.
Of course, we are not certain there is no method faster than Newton method, and implementation details of CVXPY may change the conclusion too.

We attemp to give a heuristic argument regarding the complexity of Newton steps.
We say the function being minimized is \m {\VNm {\V {g}}_1}.
Let \m {\V {g} _0} denote the starting value of \m {\V {g}}, and \m {\V {g} ^{\star}} the the point of convergence.
Then Boyd and Vandenberghe \cite [BoV04] (p.505, Eqn.9.56) gave a bound of the number of steps.
If we just take that as the complexity bound for DS, it becomes
%
\DispNum {C:S:C0:1e'} {
\NC C_{\Rm {DS}}
= \NC \RB {C_0 \VNm {\V {g}_0 -\V {g} ^{\star}}_1
+ \log_2 \log_2 \F {1} {\e}} C_{\Rm {step}} \NR
}
%
Here \m {C_0} is constant related to implementation of Newton method, and \m {\e} the tolerance of error, and \m {C_{\Rm {step}}} is the complexity of Newton step.
According to their analysis (Eqn.9.57), for most cases \m {-\log_2 \log_2 \e} can be bounded by \m {6}.
Also, \m {C_0} usually assumes the value of about several hundred.
There was an example they gave in which \m {C_0 =375}, where, with their notation,
\m {C_0 =(20 -8\a) / (\a \b \RB {1 -2\a}^2}), where \m {\a = 0.1, \b = 0.8, \e =0.01}, and \m {\a, \b} are parameters used in the Backtrack Line Tracing algorithm).

Every Newton step involves a inversion of \m {\MS {O} \SB {\M {P} ^\Adj \M {P}}}, having the same order of magnitude to the time needed for matrix multiplication.
Since the dimension of \m {\M {P} ^\Adj \M {P}} is \m {N_H^2}, the cost of inversion may be roughly considered to be \m {\MS {O} \SB {N_H^4}}, as we know from newer results on the complexity of matrix inversion.
That is, we suppose \m {C_{\Rm {step}} = N_H^4}.

We take some liberty to assume that, at the start the iteration, the least square estimator was taken to be the initial value of \m {\V {g}_0}, namely
%
\DispNum {g:0:gL:Py} {
\NC \V {g}_0
=\NC \V {g}_{\Rm {LS}} \NR
\NC = \NC \RB {\M {P} ^\Adj \M {P}} ^{-1} \M {P} ^\Adj \V {y} \NR
}
%
By above, we may write the complexity of DS to be
%
\DispNum {C:S:Og:g1} {
\NC C_{\Rm {DS}}
=\NC \MS {O} \SB {\VNm {\V {g}_{\Rm {LS}} -\V {g} ^\star} _1} N_H^4 \NR
}
%
Also assume
%
\DispNum {g:g:g;:;g} {
\NC \V {g} ^\star
\approx \NC \V {g} \NR
}
%
Indeed, this is the main thesis of our investigation.
And by restricted isometry, \m {\M {P}} has unity-normed, almost orthogonal columns, so
%
\DispNum {P:P:IN:H2} {
\NC \M {P} ^\Adj \M {P}
\approx \NC I _{N_{H}^2} \NR
}
%
Thus, by \Rf {g:0:gL:Py}, \Rf {C:S:Og:g1}, and \Rf {g:g:g;:;g},
%
\DispNum {C:S:Og:z1} {
\NC C_{\Rm {DS}}
=\NC \MS {O} \SB {
   \VNm {\V {g} +\RB {\M {P} ^\Adj \M {P}} ^{-1} \M {P} ^\Adj \V {z}
   -\V {g}} _1} N_H^4 \NR
\NC =\NC \MS {O} \SB {\VNm {\M {P} ^\Adj \V {z}} _1} N_H^4 \NR
}

We see that \m {\RB {\M {P} ^\Adj \V {z}} _{\SB {j}}} observes complex standard normal distribution.
If we agree that, for high probability,
%
\DispNum {O:j:sO:O1} {
\NC \MS {O} \SB {\VNm {\M {P} ^\Adj \V {z}} _{\SB {j}}}
= \NC \s \MS {O} \RB {1} \NR
}
Then we simply have
%
\DispNum {C:S:ON:H6} {
\NC C_{\Rm {DS}}
=\NC \MS {O} \SB {N_{H}^6} \NR
}

On the other hand, Tropp and Gilbert \cite [TrG07a] discussed the complexity of OMP in the companion paper \cite [TrG07b].
They gave
%
\DispNum {C:P:ON:NY} {
\NC C_{\Rm {OMP}}
=\NC \MS {O} \SB {N_H^2 \log N_Y} \NR
}
%
Here \m {\log N_Y} is small in our experiments, and if we dropped some constants and \m {\log N_Y}, then we may assume \m {\MS {O} \SB {N_H^2}}.

Lastly, since Lasso involves the same target function as DS, the same argument is valid, giving \m {C_{\Rm {Lasso}} =\MS {O} \SB {N_H^6}}.

\stopsubsection

\startsubsection [title={Runtime Statistics}]

Use \m {t_{\Rm {OMP}} \SB {N_H}}, \m {t_{\Rm {Lasso}} \SB {N_H}}, \m {t_{\Rm {DS}} \SB {N_H}} to denote their respective time taken.
We reproduce the arithmetic average (including different \m {\s}) of simulated values below, in seconds.
\DispNum {t:2:95:55} {
\NC t_{\Rm {OMP}} \SB {12} \NC = 0.0119,
\FourQ t_{\Rm {OMP}} \SB {16} = 0.0297, \NR
\NC t_{\Rm {OMP}} \SB {20} \NC = 0.0600,
\FourQ t_{\Rm {OMP}} \SB {24} = 0.155, \NR
%
\NC t_{\Rm {Lasso}} \SB {12} \NC = 0.0436,
\FourQ t_{\Rm {Lasso}} \SB {16} = 0.176, \NR
\NC t_{\Rm {Lasso}} \SB {20} \NC = 0.471,
\FourQ t_{\Rm {Lasso}} \SB {24} = 1.26, \NR
%
\NC t_{\Rm {DS}} \SB {12} \NC = 0.357,
\FourQ t_{\Rm {DS}} \SB {16} = 2.07, \NR
\NC t_{\Rm {DS}} \SB {20} \NC = 29.8,
\FourQ t_{\Rm {DS}} \SB {24} = 91.5, \NR
}
Simulation indeed shows that \m {t_{\Rm {DS}}} grows tremendously with (in our case) \m {N_H}, agreeing the criticism by Friedlander and Saunders \cite [FrS07] on the high complexity of DS.
It is not surprising, regarding the fact that the convex program has to find the Newton step by calculating (in our case) \m {\M {P}}, a matrix of dimension \m {\M {N}_H^2}.
The matrix inversion is probably the bottleneck of complexity.
However, as we noted above, that different tolerance level of and maximal iteration number may result in vastly different computation time.

Meanwhile, \m {t_{\Rm {Lasso}}} and \m {t_{\Rm {OMP}}} are often an order of magnitude lower than \m {t_{\Rm {DS}}}.
In general, \m {t_{\Rm {DS}} \gg t_{\Rm {Lasso}} \gg t_{\Rm {OMP}}}.
It is now wonder that, given that OMP is a greedy algorithm, the time OMP takes is negligibly small.
But it is also notable that, while Lasso has form similar to DS, it takes much lower time.

Assuming the complexity of each of three methods is polynomial time, we calculate a least square fit for the power of complexity, giving the following estimated values, on which we add a tilde.

\DispNum {t:P:NH:83} {
\NC \T {t}_{\Rm {OMP}} \NC \eqsim \MS {O} \SB {N_H^{3.6}} \NR
\NC \T {t}_{\Rm {Lasso}} \NC \eqsim \MS {O} \SB {N_H^{4.8}} \NR
\NC \T {t}_{\Rm {DS}} \NC \eqsim \MS {O} \SB {N_H^{8.3}} \NR
}

\stopsubsection

\stopsection

\stopchapter

