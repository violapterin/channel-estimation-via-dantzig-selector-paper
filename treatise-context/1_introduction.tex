\startchapter [title={Introduction}]

\startsection [title={Estimation of MIMO channel}]

Multiple-input multiple-output (MIMO) communication system will be part of the 5G specification.
With a large number of antennae on both transmitter and receiver ends, MIMO is expected to provide a large signal gain, by virtue of either parallel transmission of data, redundant transmission, or beamforming which improves the signal level.
Besides, the millimeter wave (mm-wave) is adopted, since its smaller wavelength (and thus higher frequency) makes wider bands available.
Moreover the antennae may be closer-spaced, allowing us to increase their number \cite [RSM13].
However, also because of the larger antennae array, and larger noise corruption in higher frequency, estimation of MIMO channel state information gives rise to higher complexity, and hence higher hardware overhead and power consumption.
Indeed, RF chains are more expensive and power consuming, so there is a growing attention on hybrid beamforming, where there are not fewer RF chain than the antennae.

If we consider a slow varying MIMO channel for simplicity, in terms of a channel representation, to estimate the channel is to determine the parameters of the representation.
It amounts to invert a linear system whose dimension is the number of antennae, as we shall see in chapter 2 in more detail.
In this case, it is pointed out that conventional training based algorithms are not very effective.

\stopsection

\startsection [title={Compressive sensing}]

Fortunately, physical evidence has suggested that mm-wave channel are poor in scattering \cite [ALS14], which reduces the number of paths, and it is now that a recent development called compressive sensing may help.

\blank [big]
\externalfigure [compressive-sensing.png] [wfactor=fit, hfactor=fit]
\FigureCaption {
A conceptual illustration of compressive sensing.
(Retrieved from \goto {\hyphenatedurl {Wikimedia Commons}} [url (commons.wikimedia.org/wiki/File:Orthogonal_Matching_Pursuit.gif)])
}
\blank [big]

Figure 1 shows a generic situation in which a signal is crowded in the given basis, but is sparse in transformed basis.
The black line on the left is the original signal, and the yellow line is the reconstruct signal.
If the signal is sparse on certain new basis, as shown in the right, then for a good approximation, we may take only the largest components for reconstruction.

Generally speaking, compressive sensing aims to reconstruct a underdetermined linear system, when the sparsity of solution guarantees successful recovery for most cases.
More measurements are required to recover the signal in the nonsparse basis, while fewer suffice in the sparse basis.
But, even if we are sure that the sparse basis exists, its construction is another undertaking.
In our settings, it is not trivially obvious that hybrid beamforming structure fits the compressive sensing problem, and a method to generating the sensing matrix and pilot vectors in the estimation stage has to be devised and justified.

Compressive sensing approaches can be divided into two categories, the convex programming approach, and greedy approach \cite [RDD18].
We will discuss one from each in more details: Dantzig Selector and Orthogonal Matching Pursuit.

\stopsection

\startsection [title={The Dantzig Selector}]

Let us say the number of model parameters \m {N_p \in \MB {N}} is much larger than the number of measurements \m {N_m \in \MB {N}}, that is,
%
\DispNum {N:p:Nm:Nm} {
\NC N_p \gg \NC N_m \NR
}
%
To start, consider \m {\V {x}, \V {x}' \in \MB {K} ^{N_p}} and \m {\M {Q} \in \MB {K} ^{N_m \D  N_p}}, and the linear system
%
\DispNum {y:y:Qx:Qx} {
\NC \V {y}
=\NC \M {Q} \V {x} \NR
}
Certainly, if \m {\M {Q} \RB {\V {x}_1 - \V {x}_2} = 0}, then \m {\V {x}_1, \V {x}_2} are indistinguishable.
But if \m {\V {x}} is sparse, can we rule out some of the possibilities?

To be precise, we say \m {\V {x}} is \m {s}-sparse, if only at most \m {s} components of \m {x} is nonzero.
That is, if there is some \m {\MS {A} \subseteq \CB {0, \dots, N_p-1}} such that
%
\DispNum {x:A:x::x:} {
\NC \V {x} _{\SB {\MS {A}}}
=\NC \V {x} \NR
}
%
with
%
\DispNum {A:A:s::s:} {
\NC \Nm {\MS {A}} \leq \NC s. \NR
}

It is shown that in the noiseless case, a \m {\ell_1}-minimization program recovers the \m {N_m}-dimensional signal, with \m {N_p} measurements, under overwhelming probability \cite [Can05].

As an application, it often occurs that a camera takes a high definition photo, and compresses the image later.
But it may be desirable that a camera equipped with fewer sensors shall take a lower resolution photo to save storage, and shall recover the image later.
Such advantage can be useful in medical imaging, since it is not only expensive, but also harmful, to take too many images, while the accuracy is critical to the diagnosis too \cite [CaT07].

If the linear system is moreover subject to noise \m {z},
%
\DispNum {y:y:Qx:xz} {
\NC \V {y}
=\NC \M {Q} \V {x} + \V {z} \NR
}
is it still possible to recover \m {\V {x}}?

We introduce some conventions.
For \m {\MS {A} \subseteq \CB {0, \dots, N_p-1}}, denote
%
\Disp {
\NC \V {x}  _{\SB {\MS {A}}}
=\NC \sum _{i \in \MS {A}} \V {v} \V {u} _{i} \NR
%
\NC \M {Q}  _{\SB {\MS {A}}}
=\NC \sum _{i \in \MS {A}} \M {Q} _{\SB {:,i}} \NR
}
%
That is, respectively, the components of \m {\V {x}}, and the columns of \m {\M {A}}, that have indices in \m {\MS {A}}.


\Result
{Definition}
{
For fixed \m {s =0, \dots N_p -1}, we say that \m {\M {Q}} satisfies \m {\d_s}-restricted isometry property (hereafter \m {\d_s} RIP) of sparsity \m {s} with respect to \m {0 \leq \d_s \leq 1}, if for all \m {s}-sparse \m {\V {x}}
%
\DispNum {1:2:Qx:22} {
\NC \RB {1-\d_s} \VNm {\V {x}} _2 ^2
\leq \NC \VNm {\M {Q} \V {x}} _2 ^2 \NR
%
\NC \leq \NC \RB {1+\d_s} \VNm {\V {x}} _2 ^2 \NR
}
}
%
It helps to think that \m {\M {Q}} is almost unitary up to relative error \m {\d_s}.

For concreteness, say \m {\V {z}} is an i.i.d.\ normalized random normal vector.
If so, a stronger result was established \cite [CaT07] that, with another \m {\ell_1} minimization program called Dantzig selector (DS), recovery under the noisy case is again possible under high probability.

\Result
{Algorithm}
{
\startitemize[n]
\item Input \m {\M {Q} \in \MB {R} ^{N_m \D N_p}} and \m {\V {y} \in \MB {R} ^{N_m}}.
%
\item Compute the convex program
%
\DispNum {h:h:mi:yg'} {
\NC \Hat {\V {h}}
\LA \NC \startcases
\NC \Min {\V {h}'} \MC \VNm {\V {h}'} _1 \NR
%
\NC \Rm {subject} \; \Rm {to} \Q \MC \VNm {\M {Q}^\Adj \RB {\M {Q} \V {h}' -\V {y}}} _\infty \leq \g \NR
\stopcases \NR
}
\item Output \m {\Hat {\V {h}}}.
\stopitemize
}

Algorithm [2] recovers \m {\V {x}} with the expected error norm bounded with overwhelming probability.
The \m {\ell_1}-minimization problem with \m {\ell_\infty}-constraint may be recast as a linear Program, rendering convex programming technique applicable.
This allows techniques from convex optimization to be used, and CandÃ¨s and Romberg provided an implementation on their website \cite [CaR05].

Here the constant \m {\d_s} is significant, since we can't just take \m {\M {Q}} to be any unitary matrix, for which \m {\d_s =0}.
A common approach is choosing i.i.d.\ entries of \m {\M {Q}}, and it was established by Baraniuk et.\ al.\ \cite [BDD08] that if \m {\VNm {\M {Q} \V {x}}} concentrates sharply, \m {\M {Q}} has RIP for overwhelming probability, but it remains to conceive an algorithm that efficiently generates RIP matrices.

Back to our problem of MIMO channel estimation, recall that, luckily, physical evidences suggest that mm-wave channels are sparse in the number of paths.
Bajwa et.\ al.\ \cite [BHS10] used DS, where they argued that if the \m {\ell_0}-norm of the channel matrix may be bounded by a constant, DS may be applied to estimate the time-dependent single-antenna channel response.
The accompanying note by the same researchers \cite [BHR08] showed that \m {X} has RIP for overwhelming probability, justifying their work.

Some scholars applied least absolute shrinkage and selection operator (Lasso) instead.
In fact, it can be shown that Lasso and DS have similar behavior \cite [AsR10].
Lian, Liu and Lau applied Lasso on MIMO with an analog combiner \cite [LLL17].
Destino, Juntti, and Nagaraj used an adaptive Lasso \cite [DJN15], in which they jointly optimize the sensing matrix and pilot vectors.
Vlachos, Alexandropoulos, and Thompson \cite [VAT19] used Lasso on hybrid beamforming with an additional random spatial sampling device.

\stopsection

\startsection [title={Orthogonal Matching Pursuit}]

In addition to convex programming approaches, Tropp and Gilbert \cite [TrG07b] suggested likewise that a greedy algorithm called orthogonal matching pursuit (OMP) which also aims to reconstruct a sparse signal in an underdetermined system.
Again consider the situation of \Rf {y:y:Qx:xz}.

\Result
{Algorithm}
{
\startitemize[n]
%
\item Input \m {\M {Q} \in \MB {R} ^{N_m \D N_p}} and \m {\V {y} \in \MB {R} ^{N_m}}, \m {\eta >0}.
%
\item Initialize
%
\Disp {
\NC \V {r}
\LA \NC \V {y} \NR
%
\NC S
\LA \NC \varnothing \NR
}
%
\item Start the loop with counter \m {i \LA 1}.
%
\item Find
%
\DispNum {n:n:n0:nr} {
\NC n
\LA \NC \underset {n' =0, \dots, N_p-1} {\Rm {argmax}}
\Nm {\M {Q} _{\SB {:,n'}} \V {r}} \NR
}
%
and append \m {S \LA S \cup {n}}.
%
\item Compute
%
\DispNum {Q:Q:QS:Qy} {
\NC \M {Q} ^\ddagger
\LA \NC \RB {\M {Q} _{\SB {S}} ^\Adj \M {Q} _{\SB {S}}} ^{-1} \M {Q} _{\SB {S}} ^\Adj \NR
%
\NC \V {r}
\LA \NC \V {y} -\M {Q} _{\SB {S}} ^\Adj \M {Q} ^\ddagger \V {y} \NR
}
%
\item Break if
%
\DispNum {r:2:h':h'} {
\NC \VNm {\V {r}} _2
<\NC \eta \NR
}
%
otherwise, go to step 4.
%
\item Output 
%
\DispNum {g:g:Qy:Qy} {
\NC \Hat {\V {g}}
\LA \NC \M {Q} ^\ddagger \V {y} \NR
}
\stopitemize
}
%
The columns of \m {\M {Q}} are chosen greedily, and we expect its columns to span \m {\V {x}}, playing a role similar to the sensing matrix in the case of DS.
In each step, we estimate the noise \m {\V {r}}, and find a new column most correlated with it, to refine the estimation for \m {\V {x}}.
From Tropp and Gilbert \cite [TrG07a], the probability that OMP recovers \m {\V {x}} completely is overwhelming.

OMP has since been commonly used for channel estimation.
Alkhateeb et.\ al.\ \cite [AEL14] proposed an adaptive algorithm with a beam codebook for quantized angles.
Alkhateeb, Leus, and Heath Jr.\ \cite [ALH15] examined the trade-off between number of measurement and accuracy for an all-phase-shifter beamforming matrix based on nonuniform fixed set of angles.
Hu, Wang, and He \cite [HWH13] applied OMP to estimate quantized path delay of OFDM subcarriers of stationary channel gain.
Lee, Gil, and Lee \cite [LGL16] considered a hybrid system, where the sensing matrix serves as sensing matrix, based on a designed set of angles.
Gao, Dai, and Wang proposed a variant which there is the assumption of spatially common sparsity \cite [GDW15].

Performance guarantee of OMP is also studied on various conditions.
Cai, Wang, and Xu \cite [CWX10], gave a new bound on performance of OMP under assumption of low coherence of columns of matrices, instead of almost unitarity.
Cai and Wang \cite [CaW11] extended the study to DS and other convex programs for sparse recovery, and Ben-Haim et.\ al.\ \cite [BEE10] followed up and refined their bounds, concluding that OMP is better for low SNR scenario, and DS is better for high SNR.

\stopsection

\startsection [title={Contribution}]

Due to the constraint of hybrid structure, the designs which use a fully digital beamforming cannot be directly applied, and the designs which use only analog beamforming might not be optimal.
Moreover, it appears possible that that convex programming outperforms greedy methods, and is necessary in less ideal situations.

In this treatise, we consider a hybrid structure with uniform linear array with both precoders and combiners on each side.
In chapter 2, we plan to generate random beamforming matrices, and use DS to estimate the channel in the space frequency domain.
In chapter 3, we shall see that our proposed sensing matrix has RIP for high probability, which may serve as the sensing matrix for DS,
and we give a quantitative bound (which holds for high probability) on expected error norm.
In chapter 4, numerical results show that DS is superior to other methods for our problem.
Considering its higher complexity, we remark that it can be cast as a linear program, and basis pursuit denoising form may be used.

It appears that, for given number of sampling, DS recovers more successfully than OMP.
Moreover, our setting is more general than Alkhateeb, Leus, and Heath Jr.\ \cite [ALH15] with only analog combiner,
and our random generation of sensing matrix is simpler than Lee, Gil, and Lee \cite [LGL16],
and we do not rely on additional assumption on sparsity as in Gao, Dai, and Wang \cite [GDW15].

It also is different from Bajwa et.\ al.\ \cite [BHS10];
while they require a series of random pilot vectors and consequently the sensing stage take considerable time, we require only i.i.d.\ random sensing matrices and take just a few time frames.

In addition, it seems that with the same threshold, DS is better than Lasso.
Again, our proposed method is more general than Lian, Liu and Lau \cite [LLL17] with only analog combiner,
and our sensing matrix is generated more simply than in Destino, Juntti, and Nagaraj \cite [DJN15],
and we do not require special device as in Vlachos, Alexandropoulos, and Thompson \cite [VAT19].


\stopsection

\stopchapter

