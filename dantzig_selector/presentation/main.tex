\input {./preamble.tex}


\starttext

% % % % % % % % % % % % % % % % % % % % % % % % % %

\blank [big, force]
\Title {Dantzig Selector \\ Applied on mm-Wave MIMO: \\ Part I. Expected Error Analysis}
\blank [big]
\Subtitle {Presenter: Tzu-Yu Jeng \\ Advisor: Prof.\ H.J.\ Su}
\blank [big]
\Subsubtitle{May 30, 2019}
\Subsubtitle{Graduate Instute of Commnication Engineering, \\ National Taiwan University}

\page[yes] % % % % % % % % % % % % % % % % % % % % % % % % % %

\Frametitle {Organization}

\startitemize
\I Problem Configuration Proposed Method
\I Sparsity of Angular Channel Matrix
\I Concentration Inequality of \m {\M{P}}
\I Restricted Isometry of \m {\M{P}}
\I Expected Error of Dantzig Selector
\I Conclusion and Future Work
\I References
\stopitemize

\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %

\stoptext

\Frametitle {Background}
\startitemize

\I Multiple-input-multiple-output (MIMO) wireless communication is widely expected to be the next-generation communication scheme
\I But reliable communication requires that the channel response be known at the receiver to facilitate, say, beamforming algorithm and channel calibration
\I And in the MIMO system, estimating wireless channels calls for high complexity
\I Meanwhile, in the millimeter wave r\`egime, which is often used together with MIMO, channel often exhibits sparse properties
\I If the sparsity is exploited, can few observations suffice to estimate the channel?

\stopitemize

\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %

\Frametitle {Compressive Sensing}
\startitemize

\I Recent developement on channel estimation is facilitated by advances of so-called compressed sensing (CS)
\I CS addresses the common situation in statsitical applications that the number of model parameters \m {} the number of measurements \m {}.
\I With insufficient (possibly noisy) measurements, how well can we recover all \m {} parameters?
Of course, more assumption must be made.
\I Work on CS reveals that few observations of the signal may be sufficient for us to resonstruct the signal when it is sparse.
\I In the advent, Cand\`es and Tao (2006) proposes the Dantzig Selector (DS).
It was first applied in channel estimation context

\stopitemize



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Notation}
\startitemize

\I \m {} denotes vectors
\I \m {} denotes matrices
\I \m {} refers to either \m {} or \m {}.
\I \m {} is the Hilbert space \m {} over \m {}
\I \m {} is the collection of \m {} by \m {} matrices.
\I \m {} denotes the \m {}-norm of \m {}

\stopitemize



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Definitions: Sparsity}
\startitemize

\I Suppose \m {}.
Let \m {}.
Denote as \m {} the columns of \m {} having indices in \m {}.
\I \m {} is called \m {}-sparse with support \m {}, \m {}, if only at most \m {} components of \m {} is nonzero.
\I (New def.) We say \m {} is almost-\m {}-sparse with \m {}-residue \m {}, if \m {} where \m {} is sorted in magnitude.

\stopitemize



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Program: The Dantzig Selector}
Let \m {} and \m {} be given, and \m {} be fixed.
Find \m {} with
\Disp{
\hat{\V{h}} =\min_{\V{h}'} \quad &\|\V{h}'\|_1 \\ \NT
\RM{subject}\; \RM{to} \quad &\|\M{P}^\H (\M{P} \V{h'} -\V{y})\|_\infty \leq \gamma
}



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Performance Guarantee of DS}
\startitemize

\I Now, consider a linear transformation with noise corruption,
\Disp{
\V{y} =\M{P} \V{h} + \V{z}
}
where \m {} is i.i.d.\ standard Gaussian.

\I They showed that the mean square error \m {} is bounded with overwhelming probability.

\I Furthermore, this \m {}-minimization problem with \m {}-constraint may be recast as a linear program (LP), lending convex programming technique applicable.

\stopitemize



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Definition: Restricted Isometry Property}
\startitemize

\I Consider \m {}, with unity-\m {}-norm columns.
For \m {}, we say that \m {} satisfies the restricted isometry property (RIP) of sparsity \m {} with respect to \m {}, if, for all \m {}-sparse \m {}, for all \m {} with \m {},
%
\Disp{
(1-\d_s) \|\V{x}\|^2
\leq \|P_{\MC{T}} \V{x}\|_2^2
\leq (1+\d_s) \|\V{x}\|_2^2
}
\I RIP is essentially saying that \m {} is ``almost unitary'' up to ``relative error'' \m {}.

\stopitemize



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Orthogonal Matching Pursuit}
\startitemize

\I Afterwards, scholars (Tropp and Gilbert 2007b) proposed a greedy algorithm called Orthogonal Matching Pursuit (OMP)
\I Here, we pick up the columns of the sensing matrix \m {} greedily, hoping to correspond to the support of \m {}, thus recovering the original signal.
\I An i.i.d.\ random sensing matrix may perform sufficiently well, and may even recover the original signal in an overwhemingly probability too.

\stopitemize



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Recent Literature on Compressive Channel Sensing}
\startitemize

\I Scholars has since favored OMP rather than DS, let alone other sparse algorithm, without clear justification
\I Previous work simply assumes the norm of the channel matrix is bounded in some way, and dependency on the sparsity of the channel parameters is unknown
\I OMP's requirement on the sensing matrix (elementwise i.i.d.\ Gaussian) seems to be more restrictive than DS's (RIP)
\I The quantization of angle in generating may be a problem, and that is difficult to analyze in OMP's setting

\stopitemize



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Our Work}
\startitemize

\I We will use a modified DS rather than OMP, done on the beamspace rather than the spatial domain, and involving complex numbers rather than real numbers.
\I We will show an explicit bound, where the sparsity of the virtual channel matrix is depends explicitly on the number of paths of the channel.
\I Bounding the beamspace sparsity, accordingly the concern of quantization error of the virtual channel's phase angle has been incorporated in our proof of the bound.
\I We will derive explicitly the SOCP problem and afterwards its dual problem, and wrote a proof-of-concept but efficient code.

\stopitemize



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Channel Model}
\startitemize

\I The response of uniform linear array is
\Disp{
\V{a} (\psi')
=\F{1}{\R{N_H}} \sum_{n=1}^{N} \RM{e}^{n \psi' i} \V{u}_n
\in \MB{V}_\MB{C} (N_H)
}
\I Let the virtual angle of departure and arrival be defined as thus to simplify expression
\Disp{
\f_l =\dfrac{d_{\RM{arr}}} {\lambda} \sin \f_l', \quad \th_l =\dfrac{d_{\RM{arr}}} {\lambda} \sin \th_l'
}
\I Let there be \m {} paths.
The channel matrix is, then,
\Disp{
H
=\sum_{l=0}^L \alpha_l \V{a} (\f_l) \V{a} (\th_l)^\H
\in \MB{M}_\MB{C} (N_H, N_H)
}

\stopitemize



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Precoder Setting}
\startitemize

\I We consider the hybrid configuration at both transmitter and receiver end.
In the transmitter end, there are digital precoder \m {} and analog precoder \m {}.
In the receiver end, there are digital combiner \m {} and analog combiner \m {}.
\Disp{
\M{F}_B \in &\MB{M}_{\MB{C}} (N_R, N_Y) \\
\M{F}_R \in &\MB{M}_{\MB{C}} (N_H, N_R) \\
\M{W}_R \in &\MB{M}_{\MB{C}} (N_R, N_H) \\
\M{W}_B \in &\MB{M}_{\MB{C}} (N_Y, N_R)
}
\I Recall the constraint of magnitude for analog precoders:
\Disp{
|\M{F}_R (n_h, n_r)| =1, \quad |\M{W}_R (n_r, n_h)| =1, \\
n_h =1, \dotsc N_H, \quad n_r =1, \dotsc N_R \NT
}
\I We also assume
\Disp{
N_Y \ll N_R \ll N_H
}

\stopitemize



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Effective Channel}
\startitemize

\I We have the effective channel \m {}
\I We may estimate \m {} with \m {}, \m {}, using pilog signal as unit vectors \m {}.
\I It remains to recover \m {} with knowledge of \m {}, while \m {}, \m {}, \m {}, and \m {} are in our control

\stopitemize




\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Vectorization}
Previous literature usually approaches the problem as
\Disp{
\V{h}
:= &\RM{vec} (\M{H})
\in \MB{V}_{\MB{C}} (N_g) \\
\V{y}
:= &\RM{vec} (\M{Y})
\in \MB{V}_{\MB{C}} (N_y) \\
\V{z}^\star
:= &\RM{vec} (\M{Z})
\in \MB{V}_{\MB{C}} (N_y) \\
\M{P}^\star
:= &(\M{F}_R^\Tr \M{F}_B^\Tr) \otimes (\M{W}_B \M{W}_R)
\in \MB{M}_{\MB{C}} (N_y, N_g).
}
and for short we set \m {} and \m {},
so that
\Disp{
\V{y} =\M{P}^\star \V{h} +\V{z}^\star
}



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Beamspace Channel Representation}
Let \m {} be the DFT matrix.
If we write \m {}, i.e.\ the beamspace (i.e., spatial frequency domain) representation of \m {}, then
\Disp{
\M{Y}
:=&\M{W}_B \M{W}_R \M{K} ( \M{G} \M{K}^\H \M{F}_R \M{F}_B +\M{K}^\H \M{Z} )
\in \MB{M}_{\MB{C}} (N_Y, N_Y) \\
\M{P}
:=&(\M{F}_B^\Tr \M{F}_R^\Tr \M{K}^\ast) \otimes (\M{W}_B \M{W}_R \M{K})
\in \MB{M}_{\MB{C}} (N_y, N_g)
}
where accordingly
\Disp{
\V{g} := &\RM{vec} (\M{G}) \in \MB{M}_{\MB{C}} (N_g) \\
\V{z} := &\RM{vec} (\M{K}^\H \M{Z}) \in \MB{V}_{\MB{C}} (N_y)
}
so that
\Disp{
\V{y}
=\M{P} \V{g} +\V{z}
}



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Proposed Program}
\startitemize

\I Let \m {} be given.
Then, with parameter \m {} specified, find
%
\Disp{
\hat{\V{g}}
=\min_{\V{g}'} &\|\V{g}'\|_1 \quad \\
\RM{subject}\; \RM{to}\quad
&\|\M{P}^\H (\V{y} -\M{P} \V{g}')\|_\infty \leq \gamma
}
\I And convert \m {} back to matrix form as
\Disp{
\hat{G} =\RM{vec}^{-1} (\hat{g})
}
\I And finally recover the estimated \m {} in the space domain as
\Disp{
\hat{\M{H}} =\M{K} \hat{\M{G}} \M{K}^\H.
}

\stopitemize



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Lemma: \m {} is Almost Sparse}
For arbitrary \m {}, for \m {}, \m {} is almost-\m {}-sparse with \m {}-residue \m {} to be
\Disp{
R
   \leq \F{1}{\pi} \log \F{N_H}{L}.
}



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Proof (1/3)}
\Disp{
D (\f')
:=&\left| \sum_{n=0}^{N_H-1} \RM{e}^{i n \f'} \right|
=\F{|\sin (N_H \f'/2)|}{|\sin (\f' /2)|} \\
\left| x -\F{x^3}{6} \right|
\leq &\sin x, \quad -\pi \leq x \leq \pi \\
\left| D (\f') \right|
= &\F{48}{|\f'^2 -24| |\f'|}
}
\startitemize[n]

\I Definition and arrangement
\I Can be shown with basic calculus
\I Plug the previous eqn.\ into \m {}

\stopitemize



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Proof (2/3)}
\Disp{
\M{K}^\H \V{a}(\f) (k)
=&\F{1}{N_H} D \left( \f -\F{2 \pi k} {N_H} \right) \\
R(\eta)
:=&\F{1}{N_H} \sum_{n_H =s}^{N_H -1} D \left( \eta +\F{2 \pi n_H} {N_H} \right) \\
R(\eta) -\F{2\pi} {N_H}
\leq &\F{1}{N_H} N_H \int_{\f'=2\pi L/N_H}^{2\pi} |D(\f')| d \f'
}
\startitemize[n]

\I Straightforward by definition
\I Definition
\I Approximation of rectangle to integral

\stopitemize



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Proof (3/3)}
\Disp{
R(\f)
\leq &\F{1}{2\pi} \int_{\f'=2\pi L/N_H}^{2\pi} \F{48}{(24 -\f'^2) \f'} d \f'
+\F{2\pi} {N_H} \NT \\
=&\F{1}{\pi} \log \F{2\pi N_H}{L}
-\F{1}{N_H} \log \F{4\pi^2 -24} {L^2/N_H^2 -24}
+\F{2\pi} {N_H} \\
\leq &\F{1}{\pi} \log \F{N_H}{L}.
}
\startitemize[n]

\I Plug in above bound for \m {}
\I Calculation
\I Drop the middle term (\m {}) and last term (small)

\stopitemize



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Lemma (Tentative): \m {} is Almost Sparse}
Let \m {} and \m {} be uniformly, independently distributed in \m {}.
Then \m {} is almost-\m {}-sparse with \m {}-residue \m {} to be
\Disp{
R
\leq \F{L}{\pi^2} \left( \log \F{N_H}{L} \right)^2
}





\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Lemma: RIP Probability w.r.t.\ \m {}}
Let \m {} be i.i.d.\ Radmacher, as defined in Lemma ().
Then \m {} is RIP according to \m {} for at least probability
\Disp{
1 -2 \left( \F{12}{\d} \right)^s \exp \left[ - \left( \F{\epsilon^2}{4} -\F{\epsilon^3}{6} \right) \F{\d}{2} N_p \right]
}
{\small Source: Baraniuk, Davenport, DeVore, and Wakin (2008), ``A Simple Proof of the Restricted Isometry Property for Random Matrices''}



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {The Main Bound}
Let \m {} be defined as above, then, for overwhelming probability
\Disp{
\|\V{d}\|_2
\leq \F{4}{\pi^4} \d_{3L}^4 (1-2\d_{2L}) L^4 ( \log N_H )^4
}



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Preparation for the Proof}
\startitemize

\I Use \m {} in our case
\I Let \m {} be the index set of the largest \m {} components of \m {}, \m {} be the index set of the largest \m {} position of \m {} except \m {}, and \m {}
\I Denote as \m {} the columns of \m {} having indices in \m {}.
\I Denote \m {}

\stopitemize




\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Lemmata on Vector Supports}
\startitemize

\I \m {}
\I \m {}
\startitemize

\I (modified to account nonzero \m {})

\stopitemize

\stopitemize
{\small Source: Cand\`es and Tao (2007), ``The Dantzig Selector''}



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Lemmata on RIP Matrix}
For overwhelming probability,
\startitemize

\I \m {}
\I \m {}
\startitemize

\I (generalized from real case)

\stopitemize
\I \m {}
\startitemize

\I (generalized from real case)

\stopitemize

\stopitemize
{\small Source: Cand\`es and Tao (2007), ``The Dantzig Selector''}



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Proof (1/4)}
\Disp{
\|\M{P}_{\MC{A}\MC{B}}^\Tr \M{P} \V{d}\|_2
\leq &\|\M{P}^\Tr \M{P} \V{d}\|_2 \NT \\
\leq &\R{N_g} \|\M{P}^\Tr \M{P} \V{d}\|_\infty \NT \\
\leq &4 \R{ N_g \log N_g }
}
\startitemize[n]

\I Inclusion of larger support
\I \m {} norm inequality
\I Plug in the Lemma for \m {}

\stopitemize



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Proof (2/4)}
\Disp{
\| \V{d}_{A} \|_1
\leq &\R{L} \| \V{d}_{A} \|_2 \NT \\
\leq &\R{L} \| \V{d}_{\MC{AB}} \|_2 \NT \\
\leq &\F {\d_{3L}} {1-\d_{2L}} \| \V{d}_{\MC{C}} \|_1 +\F {4} {1-\d_{2L}} \R{L N_g \log N_g} 
}
\startitemize[n]

\I \m {} norm inequality
\I Inclusion of larger support
\I Plug in the Lemma for \m {}

\stopitemize



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Proof (3/4)}
\Disp{
\| \V{d}_{\MC{C}} \|_1
\leq &\| \V{d}_{\MC{A}} \|_1 +2 \| \V{g}_{\MC{C}} \|_1 \NT \\
\leq &\F {\d_{3L}} {1-\d_{2L}} \| \V{d}_{\MC{C}} \|_1
+\F {4} {1-\d_{2L}} \R{L N_g \log N_g}
+2 \| \V{g}_{\MC{C}} \|_1 \\
\| \V{d}_{\MC{C}} \|_1
\leq &\F {4} {1-\d_{2L} -\d_{3L}} \R{L N_g \log N_g}
+2 \F{1-\d_{2L}} {1 -\d_{2L} -\d_{3L}} \| \V{g}_{\MC{C}} \|_1
}
\startitemize[n]

\I Copy the Lemma for \m {}
\I Plug in the previous eqn.\ for \m {}
\I Cancellation, simplification

\stopitemize



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Proof (4/4)}
\Disp{
\| \V{d} \|_2^2
\leq &\|\V{d}_{\MC{A}\MC{B}}\|_2^2 +\F{1}{L} \|\V{d}_{\MC{C}}\|_1^2 \NT \\
\leq & \left( \F{\d_{3L}^2} {L^2 (1-\d_{2L})^2} +\F{1}{L} \right) \|\V{d}_{\MC{C}}\|_1^2 \NT \\
&\quad +\F{8 \d_{3L}} {L (1-\d_{2L})^2} \R{N_g \log N_g} \|\V{d}_{\MC{C}}\|_1 \NT \\
&\quad +\F{16}{(1-\d_{2L})^2} N_g \log N_g \\
\|\V{d}\|_2^2
\leq &\F{1}{4 \pi^4} \d_{3L}^4 (1-2\d_{2L}) L^4 ( \log N_g )^4
}
\startitemize[n]

\I Copy the Lemma for \m {}
\I Plug in the Lemma for \m {}
\I Plug in the Lemma for \m {}, approximate using \m {}, and \m {}

\stopitemize



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Conclusion}
\startitemize[n]

\I We are concerned with effective and estimation of MIMO mm-wave channel by exploiting its sparsity
\I We adopt The Dantzig Selector (DS) rather than Orthogonal Matching Pursuit (OMP), like many of past literature, as the estimation algorithm
\I We generalized DS for complex vectors, and we carried it out in the spatial frequency domain \m {}
\I Analogous to original DS proof, we proved explicitly that the probability that the expected square error is bounded is overwhelming, where the bounding constant is an explicit function of \m {}, the number of path

\stopitemize



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {Future Work}
It remains to
\startitemize[n]

\I Find the explicit ``overwhelming'' probability
\I Transform DS into Second Order Cone Problem
\I Find the Lagrangian and dual problem
\I Write the simulation, using Newton method
\I Compare the probability bound with the simulation result

\stopitemize




\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {References}
\startitemize[n]

{ \small
\I W U Bajwa, J Haupt, G Raz, and R Nowak (2008), ``Compressed Channel Sensing'', 2008 42nd Annual Conference on Information Sciences and Systems.
\I R Baraniuk, M Davenport, R DeVore, and M Wakin (2008), ``A Simple Proof of the Restricted Isometry Property for Random Matrices'', \textit{Constructive Approximation} \textbf{28}: 253â€“263
\I S Boyd, L Vandenberghe (2004), \textit{Convex Optimization}. Cambridge U.\ Press.
\I E J Cand\`es and J Romberg (2005), ``\m {}-MAGIC: Recovery of Sparse Signals via Convex Programming'', Retrieved from \url{https://statweb.stanford.edu/~candes/l1magic/downloads/l1magic.pdf}.
}
\setcounter{countRef}{\value{enumi}}

\stopitemize



\page [yes] % % % % % % % % % % % % % % % % % % % % % % % % % %


\Frametitle {References}
\startitemize[n]

\setcounter{enumi}{\value{countRef}} { \small
\I E J Cand\`es and T Tao (2005), ``Decoding by Linear Programming'', \textit{IEEE Transactions on Information Theory}, Vol.51, No.12.
\I E Cand\`es and T Tao (2007), ``The Dantzig Selector: Statistical Estimation when \m {} is Much Larger than \m {}'', \textit{The Annals of Statistics}, Vol.35, No.6.
\I J A Tropp and A C Gilbert (2007a), ``Signal Recovery from Random Measurements via Orthogonal Matching Pursuit: The Gaussian Case'', Caltech, ACM Tech.\ Rep. Retrieved from: \url{www.acm.caltech.edu/~jtropp/reports/TG07-Signal-RecoveryTR.pdf}
\I J A Tropp and A C Gilbert (2007b), ``Signal Recovery From Random Measurements via Orthogonal Matching Pursuit'', \textit{IEEE Transactions on Information Theory}, Vol.\ 53, No.\ 12.
}

\stopitemize

\stoptext
