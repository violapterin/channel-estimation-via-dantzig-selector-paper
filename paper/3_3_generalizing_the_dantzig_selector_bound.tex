\startsection [title={Generalizing the Dantzig Selector Bound}]
\startsubsection [title={Technical Lemmata in Complex Case}]

The third and last part of our task is to plug in the almost-sparsity found in Section 3.1, and plug in the probablity found in Section 3.2.
Set for short
\PF {d;g,g}
\DispN {
\NC \V {d} 
= \NC \Hat {\V {g}} -\V {g} \NR
}
And our task is to bound the expectation \m {\MB {E} \SB {\V {d}}}.

We argue that \m {\V {g}} can essentially be seen as sparse, and for that purpose we ignore relevent terms.
Also, we do not seek a very tight bound and will approximate \m {\d_{S}} by \m {1}.
And, as shall be seen, it suffices to set
\PF {g';2,nn-h}
\DispN {
\NC \g
= \NC \R {2 \log N_h} \NR
}

The rest follows \quotation {The Dantzig Selector} very closely, but sometimes, the generalization to complex vector cannot be glossed over, and we have to examine.
To start, we state several lemmata (or their generalization) shown in their paper.

\Result
{Lemma}
{
\PF {d-K;d-A,2-g-K}
\DispN {
\NC \VNm {\V {d} _{\MC {C}}} _1
\leq \NC \VNm {\V {d} _{\MC {A}}} _1 +2\VNm {\V {g} _{\MC {C}}} _1 \NR
}
}

To show this, observe that with triangle inequality applied respectively on \m {\MC {A}} and \m {\MC {C}}, while recalling that \m {\V {g} =\V {g} _{\MC {A}}},
\PF {g,d-A,d-K}
\DispN {
\NC \VNm {\V {g}} _1
-\VNm {\V {d} _{\MC {A}}} _1
+\VNm {\V {d} _{\MC {C}}} _1
-\VNm {\V {g} _{\MC {C}}} _1
\leq \NC \VNm {\V {g} +\V {d}} _1 \NR
\NC =\NC \VNm {\hat {\V {g}}} _1 \NR
}
On the other hand, by construction that \m {\hat {g}} minimizes the \m {\ell_1}-norm,
\PF {g,d;g}
\DispN {
\NC \leq \NC \VNm {\V {g}} _1 \NR
}

\Result
{Lemma}
{
Let \m {\M {P}} be fixed, and instance of \m {\V {z}} be given.
Then the relation
\PF {E,z,P}
\DispN {
\NC \MB {E} \SB {\Nm {\IP {\V {z} , \M {P} \DB {:, n_h}}}}
\leq \NC  2 \R {\log N_h} \NR
\NC n_h 
=\NC 1, \ldots, N_h. \NR
}
holds with probability \m {\geq 1 -N_h^{-1}}.
}

Indeed, recall the fact that \m {\M {P} \DB {:, n_g}} has unity \m {\ell_2}-norm, and on the randomness of \m {\V {z}}, the quantity \m {\Nm {\IP {\V {z} , \M {P} \DB {:, n_h}}}} observes standard Normal.
Recall the following bound for \m {Q} function
\PF {Q,x;1}
\DispN {
\NC Q\SB {x}
\leq \NC \F {1} {2} \Ss {e}^{-x^2/2} \NR
}
Particularly, for \m {x =\R {2 \log N_h}},
\PF {Q,2;nn-h}
\DispN {
\NC Q\SB {\R {2 \log N_h}}
=\NC \F {1} {N_h}. \NR
}

\Result
{Lemma}
{
With the assumption that \m {\M {P}} satisfies RIP for \m {\d_S \SB {\M {P}}}, then
\PF {P,P,d}
\DispN {
\NC \VNm {\M {P}^\Adj \M {P} \V {d}} _\infty
\leq \NC  4 \R {\log N_h} \NR
}
holds with probability \m {\geq 1 -N_h^{-1}}.
}

To show this, by definition
\PF {z,y,P}
\DispN {
\NC \IP {\V {z} -\RB {\V {y} -\M {P} \hat {\V {g}}}, \M {P} \DB {:,n_y}}
= \NC \IP {\M {P} \hat {\V {g}} -\M {P} \V {g}, \M {P} \DB {:,n_y}} \NR
\NC = \NC \IP {\M {P} \V {d}, \M {P} \DB {:,n_y}} \NR
}
By construction
\PF {P,n-y,y}
\DispN {
\NC \VNm {\M {P} \DB {:,n_y}^\Adj \RB {\V {y} -\M {P} \hat {\V {g}}}} _\infty
\leq \NC \VNm {\M {P}^\Adj \RB {\V {y} -\M {P} \hat {\V {g}}}} _\infty \NR
\NC \leq \NC 2\R {\log N_h} \NR
}
By triangle inequality, together with Lemma (), we have
\PF {P,n-y,y}
\DispN {
\NC \VNm {\M {P} \DB {:,n_y}^\Adj \M {P} \V {d}} _\infty
\leq \NC \VNm {\IP {\V {z}, \M {P} \DB {:,n_y}}} _\infty
+\IP {\V {y} -\M {P} \hat {\V {g}}, \M {P} \DB {:,n_y}} \NR
\NC \leq \NC 2 \R {\log N_h} +2 \R {\log N_h} \NR
}
which implies
\PF {P,P,d} \DispN {
\NC \VNm {\M {P}^\Adj \M {P} \V {d}} _\infty
\leq \NC 4 \R {\log N_h}. \NR
}

The following two lemmata, coming from \quotation {Dantzig Selector} Lemma 1, will be stated without proof.
The generalization from the original real vector spaces to present complex ones is valid, and completely straightforward.
Note that the interpretation of magnitude and the inner product has to be changed accordingly.
\Result
{Lemma}
{
With the assumption that \m {\M {P}} satisfies RIP for \m {\d_S \SB {\M {P}}},
\PF {d-A-B;1,1}
\DispN {
\NC \VNm {\V {d} _{\MC {AB}}} _2
\leq \NC \F {1} {1-\d_{2S}} \VNm {P _{\MC {A} \MC {B}}^\Tr P d} _2 +\F {\d_{3S}} {\RB {1-\d_{2S}} \R {S}} \VNm {d_{\MC {C}}} _1 \NR
}
}
And
\Result
{Lemma}
{
\PF {d-A-B;1,1}
\DispN {
\NC \VNm {\V {d}} _2^2
\leq \NC \VNm {\V {d} _{\MC {A} \MC {B}}} _2^2 +\F {1} {S} \VNm {\V {d} _{\MC {C}}} _1^2 \NR
}
}

\stopsubsection

\startsubsection [title={Main Result}]

We are going to combine previous lemmata and show the main result.
Now, in view of the result in Section 3.1, \m {s} is free, and we set
\PF {s,1,,}
\DispN {
\NC s
=\NC 1 \NR
}
which is crude, but enough.
Accordingly,
\PF {S;L,nn-hh}
\DispN {
\NC S
=\NC L \R {\log N_H} \NR
}
Just for simplicity, we assume
\PF {d'-S;1,2}
\DispN {
\NC \d_S
\leq \NC \F {1} {2} \NR
}
and
\PF {L;4,}
\DispN {
\NC L
\geq \NC 4 \NR
}

\Result
{Theorem}
{
Let \m {\V {y}}, \m {\M {P}}, \m {\V {g}}, \m {\hat {\V {g}}}, \m {\V {d}} be defined as ()....
Then the bound
\PF {d;4,2}
\DispN {
\NC \VNm {\V {d}} _2
\leq \NC 4 \R {2} \D \R {L \log N_H} \NR
}
holds for probability \m {p}, with
\DispN {
\NC 1 -p
\leq \NC 2 \D 6^L \D \exp \SB {-\F {N_Y^2} {32}} \NR
}
}

By the definition of truncation, by \m {\ell_p} norm inequality, by \Rf {s,1,,} and \Rf {P,P,d},
\PF {P-A-B,P,d}
\DispN {
\NC \VNm {\M {P} _{\MC {A} \MC {B}}^\Tr \M {P} \V {d}} _2
\leq \NC \VNm {\M {P}^\Tr \M {P} \V {d}} _2 \NR
\NC \leq \NC \R {S} \VNm {\M {P}^\Tr \M {P} \V {d}} _\infty \NR
\NC \leq \NC 4 \R {2 L \log N_H} \NR
}

By \m {\ell_p} norm inequality, by the definition of truncation, by \Rf {d-A-B;1,1}, \PF {d'-S;1,2}, and \PF {P-A-B,P,d} just above,

\PF {d-A;L,d-A}
\DispN {
\NC \VNm {\V {d} _{\MC {A}}} _1
\leq \NC \R {L} \VNm {\V {d} _{\MC {A}}} _2 \NR
\NC \leq \NC \R {L} \VNm {\V {d} _{\MC {AB}}} _2 \NR
\NC \leq \NC 8 \R {2 \log N_H}
+\F {1} {\R{L} \RB {\log N_H} ^{1/4}} \VNm {\V {d} _{\MC {C}}} _1 \NR
}

Next, we want to use \Rf {d-K;d-A,2-g-K} to eliminate \m {\VNm {d _{\MC {A}}} _1} in \Rf {d-A;L,d-A}.
\PF {;8,2,nn-hh}
\DispN {
\NC \leq \NC 8 \R {2 \log N_H}
+\F {1} {\R{L} \RB {\log N_H} ^{1/4}} \RB {\VNm {\V {d} _{\MC {A}}} _1 +2\VNm {\V {g} _{\MC {C}}} _1} \NR
\NC \leq \NC 8 \R {2 \log N_H}
+\F {2} {\R{L} \RB {\log N_H} ^{1/4}} \VNm {\V {g} _{\MC {C}}} _1 \NR
}
And \Rf {g-C;4,p'-2} in Section 3.1 gives
\PF {d-A;8,2}
\DispN {
\NC \VNm {\V {d} _{\MC {A}}} _1
\leq \NC 8 \R {2 \log N_H}
+2\VNm {\V {g} _{\MC {C}}} _1 \NR
\NC \leq \NC \RB {16 \R {2} +\F {4} {\pi^2} \R {1 -\F {2} {\pi}}} \log N_H \NR
}

We are now in a position to bound \m {\VNm {d} _2^2}.
Using Lemma () again, we have
\DispN {
\NC \VNm {\V {d}} _2^2
\leq \NC 32 S \log N_H
+8 \d_{3S} \R {2 \log N_H} 
+\F {\d_{3S}^2} {S} \VNm {\V {d} _{\MC {C}}} _1 \NR
}
Finally, plug in the bound for \m {\VNm {\V {d} _{\MC {C}}} _1}, the last unknown, and we make use of the almost-sparsity condition of \m {\VNm {\V {d} _{\MC {C}}} _1}.
Again for simplicity, we keep only the first order small terms.
\DispN {
\NC \VNm {\V {d}} _2^2
\leq \NC 32 S \log N_H
+\F {8 \R {2\pi}} {3} \d_{3S} L \R {\F {\log N_H} {N_H}}
+\F {\pi} {9} \F {L} {s^2 N_H} \NR
}
Set \m {\d_S =1} for the worst situation, and we get the desired bound.

\stopsubsection
\stopsection


