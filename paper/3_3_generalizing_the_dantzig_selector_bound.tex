\startsection [title={Generalizing the Dantzig Selector Bound}]
\startsubsection [title={Technical Lemmata in Complex Case}]

The third and last part of our task is to plug in the almost-sparsity found in Section 3.1, and plug in the probablity found in Section 3.2.
Set for short
\PF {d:g,g}
\Disp {
\NC \V {d} 
= \NC \Hat {\V {g}} -\V {g} \NR
}
And our task is to bound its expectation.

We argue that \m {\V{g}} can essentially be seen as sparse, and for that purpose we ignore relevent terms.
Also, we do not seek a very tight bound and will approximate \m {\d_{S}} by \m {1}.
And, as shall be seen, it suffices to set
\Disp {
\NC \g 
= \NC \R {2 \log N_h} \NR
}


The rest follows \quotation {The Dantzig Selector} very closely.
The generalization to complex vector is necessary in our setting.
We spell out the proof when such generalization is nontrivial.

\Result
{Lemma}
{
Let \m {\V {g}} and \m {\V {d}} be defined as above.
Then
\Disp {
\NC \VNm {\V {d} _{\MC {K}}} _1
\leq \NC \VNm {\V {d} _{\MC {A}}} _1 +2\VNm {\V {g} _{\MC {K}}} _1 \NR
}
}

To show this, observe that with triangle inequality applied respectively on \m {\MC {A}} and \m {\MC {K}},
\Disp {
\NC \VNm {\V {g}} _1
-\VNm {\V {d} _{\MC {A}}} _1
+\VNm {\V {d} _{\MC {K}}} _1
-\VNm {\V {g} _{\MC {K}}} _1
\leq \NC \VNm {\V {g} +\V {d}} _1 \NR
}
In the first line, we recall \m {\V {g} =\V {g} _{\MC {A}}}.
On the other hand, by construction that \m {\hat {g}} minimizes the \m {\ell_1}-norm,
\Disp {
\NC \VNm {\V {g} +\V {d}} _1
=\NC \VNm {\hat {\V {g}}} _1 \NR
\NC \leq \NC \VNm {\V {g}} _1 \NR
}
The result follows by combining.

\Result
{Lemma}
{
\Disp {
\NC \MB {E} \SB {\Nm {\IP {\V {z} \SB {\o_z}, \M {P} \DB {:, n_h}}}}
\leq \NC  2 \R {\log N_h} \NR
\NC n_h 
=\NC 1, \ldots, N_h. \NR
}
with probability \m {\geq 1 -N_h^{-1}}.
}

Indeed, recall the fact that \m {\M {P} \DB {:, n_g}} has unity \m {\ell_2}-norm, and on the randomness of \m {\V {z}}.

Recall the following bound for \m {Q} function
\Disp {
\NC Q\SB {x}
\leq \NC \F {1}{2} \Ss {e}^{-x^2/2} \NR
}
Particularly, for \m {x =\R {2 \log N_h}},
\Disp {
\NC Q\SB {\R {2 \log N_h}}
=\NC \F {1}{N_h}. \NR
}

\Result
{Lemma}
{
With the condition that \m {\M {P}} satisfies RIP for \m {\d_S \SB {\M{P}}},
\Disp {
\NC \VNm {\M {P}^\Adj \M {P} \V {d}} _\infty
\leq \NC  4 \R {\log N_h} \NR
}
with probability \m {\geq 1 -N_h^{-1}}.
}

To show this, by definition
\Disp {
\NC \IP {\V {z} -\RB {\V {y} -\M {P} \hat {\V {g}}}, \M {P} \DB {:,n_y}}
= \NC \IP {\M {P} \hat {\V {g}} -\M {P} \V {g}, \M {P} \DB {:,n_y}} \NR
\NC = \NC \IP {\M {P} \V {d}, \M {P} \DB {:,n_y}} \NR
}
By construction
\Disp {
\NC \VNm {\M {P} \DB {:,n_y}^\Adj \RB {\V {y} -\M {P} \hat {\V {g}}}} _\infty
\leq \NC \VNm {\M {P}^\Adj \RB {\V {y} -\M {P} \hat {\V {g}}}} _\infty \NR
\NC \leq \NC 2\R {\log N_h} \NR
}
By triangle inequality, together with Lemma (), we have
\Disp {
\NC \VNm {\M {P} \DB {:,n_y}^\Adj \M {P} \V {d}} _\infty
\leq \NC \VNm {\IP {\V {z}, \M {P} \DB {:,n_y}}} _\infty
+\IP {\V {y} -\M {P} \hat {\V {g}}, \M {P} \DB {:,n_y}} \NR
\NC \leq \NC 2 \R {\log N_h} +2 \R {\log N_h} \NR
}
which implies
\Disp {
\NC \VNm {\M {P}^\Adj \M {P} \V {d}} _\infty
\leq \NC 4 \R {\log N_h}. \NR
}

From \quotation {Dantzig Selector} Lemma 1, first equation, we have the result as below.
The original result is for real vector spaces, but we have checked that the proof is completely valid in complex vector spaces.
The only thing that has to change accordingly is the interpretation of magnitude and the inner product, which are evident.

\Result
{Lemma}
{
With the condition that \m {\M {P}} satisfies RIP for \m {\d_S \SB {\M{P}}},
\Disp {
\NC \VNm {\V {d} _{\MC {AB}}} _2
\leq \NC \F {1}{1-\d_{2S}} \VNm {P _{\MC {A} \MC {B}}^\Tr P d} _2 +\F {\d_{3S}}{\RB {1-\d_{2S}} \R {S}} \VNm {d_{\MC {K}}} _1 \NR
}
}

From \quotation {Dantzig Selector}, Lemma 1, second equation, we have the result as below.
Again, their proof works with complex vector spaces in place of real ones too.

\Result
{Lemma}
{
\Disp {
\NC \VNm {\V {d}} _2^2
\leq \NC \VNm {\V {d} _{\MC {A} \MC {B}}} _2^2 +\F {1}{S} \VNm {\V {d} _{\MC {K}}} _1^2 \NR
}
}

\stopsubsection

\startsubsection [title={Main Result}]

We are going to combine previous lemmata and show the main result.

In accordance with (), set
\Disp {
\NC S
=\NC s^2 L \NR
}
If we set again \m {s =\R{N_H}}, we get \m {S =N_H L}.

\Result
{Theorem}
{
Let \m {\V {y}}, \m {\M {P}}, \m {\V {g}}, \m {\hat {\V {g}}}, \m {\V {d}} be defined as above.
Then the bound
\Disp {
\NC \VNm {\V {d}} _2
\leq \NC 4 \R{2} \D \R{L N_H \log N_H} \NR
}
holds for probability \m {p}, with
\Disp {
\NC 1 -p
\leq \NC 2 \D 6^S \D \exp \SB {-\F{N_Y^2} {32}} \NR
}
}


The first quantity we want to bound away is
\Disp {
\NC \VNm {\M {P} _{\MC {A} \MC {B}}^\Tr \M {P} \V {d}} _2
\leq \NC \VNm {\M {P}^\Tr \M {P} \V {d}} _2 \NR
\NC \leq \NC \R {S} \VNm {\M {P}^\Tr \M {P} \V {d}} _\infty \NR
\NC \leq \NC 4 \R {S \log N_h} \NR
}
Thus, by virtue of Lemma (), and approximating \m {1 /(1 -\d_{2S}) =1 +\d_{2S}} and so on,
\Disp {
\NC \VNm {\V {d}_{\MC {A}}} _1
\leq \NC \R {S} \VNm {\V {d}_{\MC {A}}} _2 \NR
\NC \leq \NC \R {S} \VNm {\V {d} _{\MC {AB}}} _2 \NR
\NC \leq \NC \d_{3S} \RB {1+\d_{2S}} \VNm {\V {d} _{\MC {K}}} _1
+4 \RB {1+\d_{2S}} S \R {\log N_h} \NR
}

Next, we must eliminate \m {\VNm {d _{\MC {A}}} _1} in rhs of Lemma ().
\Disp {
\NC \VNm {\V {d} _{\MC {K}}} _1
\NC \leq \d_{3S} \RB {1+\d_{2S}} \VNm {\V {d} _{\MC {K}}} _1
+4 \RB {1+\d_{2S}} S \R {\log N_h}
+2 \VNm {\V {g} _{\MC {K}}} _1 \NR
}
Arranging and drop second order small terms like \m {\d_{2S} \d_{3S}}, that is,
\Disp {
\NC 1 \ll \NC S \NR
\NC \ll \NC N_h \NR
}
doing this we have
\Disp {
\NC \VNm {\V {d} _{\MC {K}}} _1
\leq \NC 2 \RB {1+\d_{3S}} \VNm {\V {g} _{\MC {K}}} _1
+4 \RB {1+2\d_{3S}} S \R {\log N_h} \NR
}

We are now in a position to bound \m {\VNm {d} _2^2}.
Using Lemma () again, and only keeping to first order, we have
\Disp {
\NC \VNm {\V {d}} _2^2
\leq \NC 16 S \log N_h
+8 \d_{3S} \R {\log N_h} 
+\F {\d_{3S}^2} {S} \VNm {\V {d} _{\MC {K}}} _1 \NR
}
Finally, plug in the bound for \m {\VNm {\V {d} _{\MC {K}}} _1}, the last unknown, and we make use of the almost-sparsity condition of \m {\VNm {\V {d} _{\MC {K}}} _1}.
Again for simplicity, we keep only the first order small terms.
Plug in \m {S =s^2 L} and \m {N_h =N_H^2} we get, finally,
\Disp {
\NC \VNm {\V {d}} _2^2
\leq \NC 32 S \log N_H
+\F {8 \R {2\pi}} {3} \d_{3S} L \R {\F {\log N_H} {N_H}}
+\F {\pi} {9} \F {L} {s^2 N_H} \NR
}
Set \m {\d_S =1} for the worst situation, and we get the desired bound.

\stopsubsection
\stopsection


