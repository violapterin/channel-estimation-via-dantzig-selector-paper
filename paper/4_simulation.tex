\startchapter [title={Simulation}]

We are ready to verify theoretical bound of DS by numerical experiments.
DS is a convex optimization program, and there are many library devoted on that.
For present purpose, we choose the ready-made Python library CVXPY.
For the implementation details, see Diamond and Boyd \cite [DiB16]

However, it turns out direct implementation of () exhibit extraordinarily large complexity, and we would like to recast the problem in a more tractable form.
Towards that end, we found out that Candès and Romberg \cite [CaR05] has shown that DS can be simplified as an linear program (LP).
However, the situation is more cubersome than that, since they deal with real vectors, but we consider the complex ones.
Fortunately, they have briefly pointed out, complex DS can be recast as an second-order cone program (SOCP), and SOCP is one of the several standard convex programming problems for which there are already numerous efficient algorithms.

Before describing the recast, for ease of treatment we would like to represent complex vector and matrices by real, extended vectors and matrices.
\Result
{Definition}
{
For \m {\V {x} \in \MB {C} ^{M}}, define the real representation \m {\MC {R} \SB {\V {x}}} of \m {\V {x}} to be
%
\Disp {
\NC \MC {R} \SB {\V {x}}
\in \NC \MB {R} ^{2M} \NR
\NC \MC {R} \SB {\V {x}} _{\SB {m}}
= \NC \startcases
\NC \MF {Re} \SB {\V {x} _{\SB {m'}}}, \MC m =2m' \NR
\NC \MF {Im} \SB {\V {x} _{\SB {m'}}}, \MC m =2m'+1 \NR
\stopcases \NR
\NC m' 
= \NC  0, 1, 2, \ldots, M-1 \NR
}
%
The injectivity is obvious, and we may define \m {\MC {R} ^{-1}} so that
%
\Disp {
\NC \MC {R} ^{-1} \SB {\MC {R} \SB {\V {x}}}
=\NC \V {x} \NR
}
}
%
Accordingly, the following generalization to complex matrices is valid, once we call the ring representation of complex numbers.
\Result
{Definition}
{
For \m {\M {A} \in \MB {C} ^{M_1 \D M_2}}, define real representation \m {\MC {R} \SB {\M {A}}} of \m {\M {A}} to be
%
\Disp {
\NC \MC {R} \SB {\M {A}}
\in \NC \MB {R} ^{2M_1 \D 2M_2} \NR
\NC \MC {R} \SB {\M {A}} _{\SB {m_1,m_2}} =
\NC \startcases
\NC \MF {Re} \SB {\M {A} _{\SB {m_1',m_2'}}}, \MC (m_1, m_2) = (2m_1', 2m_2') \NR
\NC \MF {Im} \SB {\M {A} _{\SB {m_1',m_2'}}}, \MC (m_1, m_2) = (2m_1'+1, 2m_2') \NR
\NC -\MF {Im} \SB {\M {A} _{\SB {m_1',m_2'}}}, \MC (m_1, m_2) = (2m_1', 2m_2'+1) \NR
\NC \MF {Re} \SB {\M {A} _{\SB {m_1',m_2'}}}, \MC (m_1, m_2) = (2m_1'+1, 2m_2'+1) \NR
\stopcases \NR
\NC m_1 
= \NC 0, 1, 2, \ldots, M_1 -1 \NR
\NC m_2 
= \NC 0, 1, 2, \ldots, M_2 -1 \NR
}
}

With these, we define
%
\Disp {
\NC \T {\V {y}}
= \NC \MC {R} \SB {\V {y}}
\in \MB {R} ^{2N_y} \NR
%
\NC \T {\V {g}}
= \NC \MC {R} \SB {\V {g}}
\in \MB {R} ^{2N_h} \NR
%
\NC \T {\M {P}}
= \NC \MC {R} \SB {\M {P}}
\in \MB {R} ^{2N_y \D 2N_h} \NR
%
\NC \T {\M {P}} ^\Adj
= \NC \MC {R} \SB {\M {P} ^\Adj}
\in \MB {R} ^{2N_y \D 2N_h} \NR
%
\NC \T {\V {z}}
= \NC \MC {R} \SB {\V {z}}
\in \MB {R} ^{2N_y} \NR
}
%
such that, by construction,
%
\Disp {
\NC \V {\T {y}}
= \NC \M {\T {P}} \V {\T {g}} +\V {\T {z}} \NR
}

Furthermore, introduce the following matrices to identify the components where we want to take \m {\ell_2}-norm, in the manner similar to an indicator function.
%
\Disp {
\NC {\V {u}}_{n_h}
\in \NC \MB {R} ^{N_h} \NR
\NC \RB {\V {u}_{n_h}} _{\SB {n_h'}}
= \NC
\startcases
1, \Q \MC n_h' =n_h \NR
0, \Q \NC \Rm {otherwise} \NR
\stopcases \NR
\NC \T {\M {U}}_{n_h} \in \NC \MB {R} ^{2N_h \D 2N_h} \NR
\NC \RB {\T {\M {U}}_{n_h}} _{\SB {n_h', n_h''}}
= \NC
\startcases
1, \Q \MC n_h' =2n_h,\; n_h'' =2n_h \NR
1, \Q \MC n_h' =2n_h +1,\; n_h'' =2n_h +1 \NR
0, \Q \NC \Rm {otherwise} \NR
\stopcases \NR
\NC n_h, n_h', n_h'' 
= \NC 0, 1, 2, \ldots, N_h -1 \NR
}
%
And denote \m {\V {1}} to be the all-\m{1} vector for short, and \m {\V {0}} the all-\m{0} vector, whose dimension will be specified from context.
Now, if we introduce vector \m{\V {m}}
%
\Disp{
\NC \V {m} \in \NC \MB {R} ^{N_h} \NR
\NC \V {m} \DB{n_h}
= \NC \Nm{\V {g} \DB{n_h}} \NR
\NC n_h, n_h', n_h'' 
= \NC 0, 1, 2, \ldots, N_h -1 \NR
}
%
to denote the entrywise complex modulus of \m{\V {g}}, we see that the convex optimization now takes the form
%
\Disp {
\NC \Hat {\T {\V {g}}}
= \NC \startcases
\NC \Min {\T {\V {g}}', \V {m}} \Q
\MC \IP { \V {1}, \V {m} } \NR
\NC \Rm {subject} \; \Rm {to} \Q
\MC \VNm { \T {\M {U}}_{n_h} \T {\V {g}}' }_2
\leq \IP { \V {u}_{n_h}, \V {m} } \NR
\NC \MC \VNm { \T {\M {U}}_{n_h} \T {\M {P}}^\Adj \RB { \T {\M {P}} \T {\V {g}}' -\T {\V {y}} } }_2
\leq \g \NR
\stopcases \NR
\NC n_h 
= \NC 0, 1, 2, \ldots, N_h-1 \NR
}

\stopsection

\startsection [title={Second Order Cone Programming}]

We shall see furthermore that the program can be cast into an SOCP.
To make the point clearer, we rewrite Program () into an extended block matrix form, by defining auxiliary variables as follows,
%
\Disp{
\NC \V {t}
= \NC \startTheMatrix
\NC \V {0} \NR
\NC \V {1} \NR
\stopTheMatrix
\in \MB {R} ^{3N_h} \NR
%
\NC \V {x}'
= \NC \startTheMatrix
\NC \T {\V {g}}' \NR
\NC \V {m} \NR
\stopTheMatrix
\in \MB {R} ^{3N_h} \NR
}
%
with
%
\Disp{
\NC \M {A}_i
= \NC \startTheMatrix
\NC \T {\M {U}}_{i}, \NC \M {0} \NR
\stopTheMatrix
\in \MB {R} ^{2N_h \D 3N_h} \NR
%
\NC \V {b}_i
= \NC \V {0}
\in \MB {R} ^{2N_h} \NR
%
\NC \V {c}_i
= \NC \startTheMatrix
\NC \V {0} \NR
\NC \V {u}_{i} \NR
\stopTheMatrix
\in \MB {R} ^{3N_h} \NR
%
\NC d_i
= \NC 0 \NR
%
\NC i 
= \NC 0, 1, 2, \ldots, N_h -1 \NR
}

And
%
\Disp{
\NC \M {A}_i
= \NC \startTheMatrix
\NC -\T {\M {U}}_{i -N_h} \T {\M {P}}^\Adj \T {\M {P}}, \NC \M {0} \NR
\stopTheMatrix
\in \MB {R} ^{2N_h \D 3N_h} \NR
%
\NC \V {b}_i
=\NC \T {\M {U}}_{i -N_h} \T {\M {P}}^\Adj \T {\V {y}}
\in \MB {R} ^{2N_h} \NR
%
\NC \V {c}_i
= \NC \V {0}
\in \MB {R} ^{3N_h} \NR
%
\NC d_i
= \NC \g \NR
%
\NC i 
= \NC N_h, N_h +1, N_h +2, \ldots, 2N_h -1 \NR
}
%
The step that we convert \m {\Hat {\V {g}}} back is same as Algorithm () is same as before, but this is listed for sake of completeness.

\Result
{Algorithm}
{
\startitemize[n]
\item Input \m{\M {P} \in \MB {C} ^{N_y \D N_h}}, \m{\V {y} \in \MB {C} ^{N_y}}, \m{\g > 0}.
\item Define \m{\V {t}, \V {x}, \M {A}_i, \V {b}_i, \V {c}_i, d_i} according to ().
\item Calculate
%
\Disp{
\NC \Hat {\V {x}}
\LA \NC \startcases
\NC \Min {\V {x}' \in \MB {C} ^{N_h}}
\MC \IP {\V {t}, \V {x}'} \NR
\NC \Rm {subject} \; \Rm {to}
\Q  \MC \VNm {\M {A} _{i} \V {x}' +\V {b} _{i}} _2
\leq \IP {\V {c}_{i}, \V {x}'} +\V {d}_i \NR
\NC \MC i 
=0, 1, 2, \ldots, 2N_h -1 \NR
\stopcases \NR
}
\item Extract
%
\Disp{
\NC \Hat{\T{\V {g}}}
\LA \NC \Hat{\V {x}} _{\SB {0 : 2N_h-1}} \NR
}
\item Convert 
%
\Disp{
\NC \Hat{\V {g}}
\LA \NC \MC{R} ^{-1} \SB {\Hat{\T{\V {g}}}} \NR
}
\item Calculate
%
\Disp {
\NC \Hat {G}
\LA \NC \Rm {vec}^{-1} \SB {\Hat {g}} \NR
}
\item Calculate
%
\Disp {
\NC \Hat {\M {H}}
\LA \NC \M {K} \Hat {\M {G}} \M {K}^\Adj \NR
}
\item Output \m {\Hat {\M {H}}}.
\stopitemize
}

\stopsection

\startsection [title={Result}]

\startsubsection [title={Settings}]




We do three series of experiments, which are of size from small to large.
In three experiments, we take respectively \m {L} to be \m {2, 3, 4}, \m {N_Y} to be \m {5, 7, 9}, \m {N_R} to be \m {10, 14, 18}, and \m {N_H} to be \m {20, 28, 36}.

Other parameters are fixed in these experiments; 
The number of grid of quantization of analog precoder is 16
The wavelength of the EM wave that antenna is emitting, \m {\l _{\Rm{ant}} = 1}, is set to be 0.1.
and the antenna spacing \m {d _{\Rm{ant}} =3}.
Note that only their ratio matters.
See chapter 2 for meaning of them.

There are also parameters related to the convex optimization algorithm.
The tolerance of absolute error in the primal-dual algorithm used in CVXPY is set as \m {10^{-4}}, and the tolerance of relative error, as \m {10^{-3}}.
The maximal number of iteration of CVXPY is set to be \m {32}.
On the other hand, the maximal number of iteration of OMP is set to be \m {4 N_H}.

Several threshold values occur in relevant alorithms.
The threshold used in DS program is called \m {\g_{\Rm {DS}}} here, and another threshold of Lasso, called \m {\a}.
Yet another threshold used for the residue norm used in OMP program is called \m {\eta}.
Various several values are tried for them, determined as thus: Take a central value roughly corresponding to the theory, and mutiply or divided the central value by powers of 2, or 

Concretely, we set \m {\g_{\Rm {DS}} = 2 \R {\log N_H}}, corresponding to Cand\`es \& Tao (2007)
For comparison, \m {\g_{\Rm {Lasso}} = 2 \R {\log N_H}} too.
For OMP, suggested values in Cai \& Wang (2011) is illuminating, where they (in their Theorem 7) considered \m {\ell _2}-norm, and Theorem 8, \m {\ell _\infty}-norm.
We make simplification and set respectively \m {\R {3} N_Y} and \m {2 \R {\log N_H}}

Each data point is repeated for 32 times, and taken average.
We plot the absolute log error
%
\DispNum {e',s;lo,vg} {
\NC \e_{\Rm{abs}}
=\NC \RB {\log_2 {\VNm {\V {h} -\Hat {\V {h}}}_2}} _{\Ss{avg}}, \NR
}
%
and relative log error.
%
\DispNum {e',l;lo,vg} {
\NC \e_{\Rm{rel}}
=\NC \F {\RB {\log_2 {\VNm {\V {h} -\Hat {\V {h}}}_2}} _{\Ss{avg}}}
{\RB {\log_2 {\VNm {\V {h}}_2}} _{\Ss{avg}}} \NR
}
%
The averaged error values are plotted along with the theoretical expected error of DS, derived in chap.\ 3, namely
%
\DispNum {e',h;52,NH} {
\NC \e_{\Rm{th}}
=\NC \F{5}{2} + \log \RB{L N_H} + \log \log N_H \NR
}

\stopsubsection

\startsubsection [title={Plots}]

\stopsubsection

\startsubsection [title={Discussion}]

Reasonably, for all methods, a higher \m {\s} corresponds to higher \m {\eta_{\Rm{rel}}} and \m {\eta_{\Rm{rel}}}.
The trends of absolute and relative error are the same, and in the below we call them simply error.

From our simulation result, DS has usually the least error.
For low SNR it is slightly but steadily outperforms OMP, and for high SNR the difference becomes vanishingly small and is sometimes surpassed by OMP.
Different values of threshold \m {\g_{\Rm{DS}}} matter, and a smaller \m {\g_{\Rm{DS}}} does not necessarily give a lower error.
It seems \m {\g_{\Rm{DS}}} is usually best chosen in the order of magnitude of \m {\s}, which is intuitive.

The error of OMP is slightly higher than DS, but the trend is less steady as well.
Interestingly, different values of threshold \m {\eta} do not seem to affect much to performance, and often the iteration stablizes much earlier than is constrained by maximum iteration number.
Lasso has substantially higher error than OMP, and Least Square (pseudo inverse) is still worse.

Lastly, the theoretical bound for expected error of DS is much looser than the actual perfomance, and is well above the Least Square line, making it the highest line of all.
Our bound is correct but not tight enough.

\stopsection

\startsection [title={Complexity}]

\startsubsection [title={Asymtotic Analysis}]

We discuss the complexity of DS and OMP.

For DS, we note that, in general, complexity analysis of convex programs is difficult, in that it is not easily ensured how many iteration the program uses.

There is an analysis in \cite [BoV04] of inequality constrained Newton method, in which they assumed self-concordance (discussed in p.496ff there, see also p.531).
It is mentioned in Candès and Romberg \cite [CaR05] that the complex DS can be cast into an Second Order Cone Programming (SOCP), and it is clear that self concordance applies to SOCP.
However, it is not clear that Newton method is the fastest, nor that they are utilized in CVXPY.
Specifically, the implementers of CVXPY, Diamond and Boyd \cite [DiB16], alleges that the program is usually converted in to primal-dual problem, and there are several different approaches in solving a primal-dual problem.
But for our purpose, we only discuss the complexity analysis for Newton method.

The function being minimized is \m {\VNm {\V {g}}_1}.
Let \m {\V {g} _0} denote the starting value of \m {\V {g}}, and \m {\V {g} ^{\star}} the the point of convergence.
Then \cite [BoV04] (p.505, eq.9.56) gives a bound of the number of iteration of Newton method.
If just take that as the complexity bound for DS, it becomes
%
\DispNum {C,S;C0,1e'} {
\NC C_{\Rm{DS}}
= \NC C_0 \VNm {\V {g}_0 -\V {g} ^{\star}}_1
+ \log_2 \log_2 \F {1} {\e} \NR
}
%
where \m {C_0} is constant related to implementation of Newton method, and \m {\e} the tolerance of error.
According to the analysis of \cite [BoV04] (eq.9.57), for most cases \m {-\log_2 \log_2 \e} can be bounded by \m {6}.
Also, \m {C_0} usually assumes the value of about several hundred;
They gives an example \m {C_0 =375} (with their notation, \m {C_0 =\dfrac {20 -8\a} {\a \b \RB {1 -2\a}^2}}, where \m {\a = 0.1, \b = 0.8, \e =0.01}, and \m {\a, \b} are parameters used in the Backtrack Line Tracing algorithm).

Since we are doing big-O analysis, we feel that we have more license on making approximation.
For concreteness, we may assume that we start the iteration by specifying the Least Square estimator as the initial value of \m {\V {g}_0}, namely
%
\DispNum {g,0;gL,Py} {
\NC \V {g}_0
=\NC \V {g}_{\Rm {LS}} \NR
\NC = \NC \RB {\M {P} ^\Adj \M {P}} ^{-1} \M {P} ^\Adj \V {y} \NR
}
%
If we drop it, we may write the complexity of DS to be, simply,
%
\DispNum {C,S;Og,g1} {
\NC C_{\Rm{DS}}
=\NC \MC {O} \SB {\VNm {\V {g}_{\Rm {LS}} -\V {g} ^\star} _1} \NR
}
%
For simplicity we assume
%
\DispNum {g-;g-,-g} {
\NC \V {g} ^\star
\approx \NC \V {g} \NR
}
%
Indeed, this is the main objective of our investigation!
And by restricted isometry, \m {\M {P}} has unity-normed, almost orthogonal columns, so
%
\DispNum {P,P;IN,H2} {
\NC \M {P} ^\Adj \M {P}
\approx \NC I _{N_{H}^2} \NR
}
%
Thus
%
\Disp {
\NC C_{\Rm{DS}}
=\NC \MC {O} \SB {
   \VNm {\V {g} +\RB {\M {P} ^\Adj \M {P}} ^{-1} \M {P} ^\Adj \V {z}
   -\V {g}} _1} \NR
\NC =\NC \MC {O} \SB {\VNm {\M {P} ^\Adj \V {z}} _1} \NR
}

If we agree that, for high probability,
%
\Disp {
\NC \MC {O} \RB {\VNm {\M {P} ^\Adj \V {z}} _{\SB {j}}}
= \NC \s \MC {O} \RB {1} \NR
}

By condition (), it is easy to see that \m {\RB {\M {P} ^\Adj \V {z}} _{\SB {j}}} observes Complex Standard Normal distribution.
and is easily bounded by \m {\s}.
%
\DispNum {--;ON,2s} {
\NC C_{\Rm{DS}}
=\NC \MC {O} \SB {N_{H}^2 \s} \NR
}

On the other hand, Tropp and Gilbert \cite [TrG07a] discussed the complexity of OMP, and the proof of technical lemmata in it can be found in the companion paper \cite [TrG07b].
They give
%
\DispNum {C,O;ON,NY} {
\NC C_{\Rm{OMP}}
=\NC \MC {O} \SB {N_H^2 \log N_Y} \NR
}
%
And it would appear that this is greater than \m {C_{\Rm{DS}}}.
However, \m {\log N_Y} is really small in our experiments.
and we dropped some constants, too, in analyzing \m {C_{\Rm{DS}}}.
If we drop \m {\log N_Y}, then we get \m {\MC {O} \SB {N_H^2}} again.

In summary
%
\DispNum {C,P;CD,H2} {
\NC C_{\Rm{OMP}}
\approx \NC C_{\Rm{DS}} \NR
\NC \approx \NC \s \MC {O} \SB {N_H^2} \NR
}

\stopsubsection

\startsubsection [title={Runtime Statistics}]

Besides analytical bound, our result shows that DS indeed exhibits high complexity when (in our case) \m {N_H} grows, and the such rate of growth is significant, 
Such computation cost is not suprising, even without knowing implementation details, from the nature of convex programming, being an iterative and random alrogithm.
Moreover, DS requires a sizable memory for holding not only (in our case) \m {\M {P}}, and matrix multiplication of dimension \m {\M {N}_H^2}.
Friedlander and Saunders \cite [FrS07] criticizes, according to their numerical experiments, that DS often gives a higher computational cost, compared to Lasso, corroborating our observation, 
However we note that different settings of \m {\z}, tolerance of error, or maximal iteration number result in vastly different computation time.

On the other hand, we see the complexity of OMP is several order of magnitude lower than that of DS.
This is reasonable, given that it is a greedy alrogithm.
In fact, the time OMP takes is negligibly small.

Since LS involves the same target function as DS, the same argument also gives \m {C_{\Rm{Lasso}} =\MC {O} \SB {N_H^2}}.
The numerical 
We are not going to further discuss Lasso.


\stopsubsection

\stopsection

\stopchapter
