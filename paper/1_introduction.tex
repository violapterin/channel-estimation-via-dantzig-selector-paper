\startchapter [title={Introduction}]

\startsection [title={The Channel Sensing Problem}]

Multiple-input multiple-output (MIMO) communication system has been proposed to be incorporated into the 5G specification (Rappaport et.\ al.\ \cite [RSM13]).
With a large number of antennae of both transmitter and receiver sides, it is expected to provide a large signal gain.
However, its hardware overhead increases complexity and power consumption, and new precoders, and algorithms, are being invented to deal with this.
Especially, millimeter waves (hereafter mm-wave), with smaller wavelength, leads to higher frequency bands, and thus wider bands to be available.
In addition, the smaller closer spaced antennae makes it possible to increase the number of antennae.
This being said, with more antennae present, conventional training-based algorithms also increase in complexity and storage.

The design of new algorithms that addressed these issues is thus necessary, and ideally, we would like to know the expression of channel in advance.
The channel capacity is the major information theoretic metric to be considered in a communication system.
Roughly, the capacity of a MIMO channel at perfect channel state information is well-known to be in the form \m {\log \det \RB {I +\Ss {snr}}}, but the expression of channel is hardly always available, and in fact, the explicit dependence of capacity on relevant parameters is often a difficult problem (Goldsmith et.\ al.\ \cite [GJJ03]).
When the estimated channel is compromised, the ensuing analysis is also undermined.

It turns out that not only is the channel hard to estimate due to a large number of antennae, but the transmission is prone to noise corruption.
In real applications, channel may also be fast varying with respect to time, and a high complexity algorithm is surely less than being ideal.

\stopsection
\startsection [title={The Dantzig Selector}]

It is a recent developement that the estimation of channels is facilitated by advances of the so-called \It {compressive sensing}.
A series of paper by Candès and Tao marks its advent.
They start by observing that it is a common situation in statsitical applications that the number of model parameters \m {N_p \in \MB {N}} is much larger than the number of measurements \m {N_m \in \MB {N}}, namely,
%
\DispNum {N:p:Nm:Nm} {
\NC N_p \gg \NC N_m \NR
}
%
As a practical example, it may be desirable that a camera equipped with few sensors may obtain a lower resolution image, to save computation and storage, and recover the image later, rather than taking a high definition photo at first, and compressing the image later.
Another example, to quote \cite [CaT07], is that of medical imaging: surely, since is not only expensive, but also harmful to the body, we may well obtain one image only, while the result can be critically important to assess the patient's well being.

But with insufficient measurements, possibly even with corruption of noise, do we have the knowledge of all \m {N_p} parameters?
Of course, more assumption must be made to make the question meaningful, since any two vectors of parameters differing an element in the null space of the sensing matrix will produce the same measurement.

They showed that in the noiseless case \cite [Can05], it is possible to recover the sparse signal, under a \m {\ell_1}-minimization program, revealing the phenomenon that fewer than \m {N_p} measurement suffices to resonstruct the \m {N_m}-dimensional signal, when it is sparse.
Later they established a stronger result \cite [CaT07] that, with another \m {\ell_1} minimization program they named The Dantzig Selector (hencefore DS), they can recover the noisy case too, but in doing so, we have to make a few carefully constructed, and seemingly random measurements.

From the computational perspective, the formulation as an \m {\ell_1} minimization problem, which is convex, made techniques from convex optimization usable.
Indeed, relevent code has been put on the web for the reader to access and verify (Candès \& Romberg \cite [CaR05]).
In this paper we focus on DS, and we introduce several notions before we describe their work.

To start, consider \m {\V {x} \in \MB {K} ^{N_p}} and \m {\M {Q} \in \MB {K} ^{N_m \D  N_p}}.
For \m {\MS {A} \subseteq \CB {0, \dots, N_p-1}}, denote
%
\Disp {
\NC \V {x}  _{\SB {\MS {A}}}
=\NC \sum _{i \in \MS {A}} \V {v} \V {u} _{i} \NR
%
\NC \M {Q}  _{\SB {\MS {A}}}
=\NC \sum _{i \in \MS {A}} \M {Q} _{\SB {:,i}} \NR
}
%
That is, respectively, the components of \m {\V {x}}, and the columns of \m {\M {A}}, that have indices in \m {\MS {A}}.

We say \m {\V {x}} is \m {s}-sparse, if only at most \m {s} components of \m {x} is nonzero.
Formally,
\Result
{Definition}
{
\m {\V {x}} is called \m {s}-sparse, if there is some \m {\MS {A} \subseteq \CB {0, \dots, N_p-1}} such that
%
\DispNum {x:A:x::x:} {
\NC \V {x} _{\SB {\MS {A}}}
=\NC \V {x} \NR
}
%
with
%
\DispNum {A:A:s::s:} { 
\NC \Nm {\MS {A}} \leq \NC s. \NR
}
}

And for given \m {s},
\Result
{Definition}
{
For fixed \m {s =0, \dots N_p -1}, we say that \m {\M {Q}} satisfies \m {\d_s}-restricted isometry property (hereafter \m {\d_s}-RIP) of sparsity \m {s} with respect to \m {0 \leq \d_s \leq 1}, if for all \m {s}-sparse \m {\V {x}}
%
\DispNum {1:2:Qx:22} {
\NC \RB {1-\d_s} \VNm {\V {x}} _2 ^2
\leq \NC \VNm {\M {Q} \V {x}} _2 ^2 \NR
%
\NC \leq \NC \RB {1+\d_s} \VNm {\V {x}} _2 ^2 \NR
}
}
%
It helps to think that \m {\M {Q}} is almost unitary up to relative error \m {\d_s}.

Now, consider a linear transformation of real vectors added of noise,
%
\DispNum {y:y:Qx:xz} {
\NC \V {y}
=\NC \M {Q} \V {x} + \V {z} \NR
}
%
where, for example, \m {\V {z}} is an i.i.d.\ normalized Gaussian vector.
How can we hope to estimate \m {\V {x}} when, in addition to insufficient measurements, there is also noise corruption?

\Result
{Algorithm}
{
\startitemize[n]
\item Input \m {\M {Q} \in \MB {R} ^{N_m \D N_p}} and \m {\V {y} \in \MB {R} ^{N_m}}.
%
\item Calculate
%
\DispNum {h:h:mi:yg'} {
\NC \Hat {\V {h}}
\LA \NC \startcases
\NC \Min {\V {h}'} \MC \VNm {\V {h}'} _1 \NR
%
\NC \Rm {subject} \; \Rm {to} \Q \MC \VNm {\M {Q}^\Adj \RB {\M {Q} \V {h}' -\V {y}}} _\infty \leq \g \NR
\stopcases \NR
}
\item Output \m {\Hat {\V {h}}}.
\stopitemize
}

Candès and Tao was able to show that, by Algorithm [3], \m {\V {x}} can be recovered, with the expected square error bounded with overwhelming probability.
Also, the \m {\ell_1}-minimization problem with \m {\ell_\infty}-constraint may be recast as a linear program (hereafter LP), lending convex programming technique applicable.
The constant \m {\d_s} in Definition [2] is crucial, and here we can't just take \m {\M {Q}} to be any unitary matrix, for which \m {\d_s =0}.
One common design choice of \m {\M {Q}} proposed is that every entry of \m {\M {Q}} shall be i.i.d., and it can be established that if \m {\VNm {\M {Q} \V {x}}} concentrates sharply, \m {\M {Q}} has RIP for overwhelming probability (Baraniuk et.\ al.\ \cite [BDD08]).
Still, it remains unclear how to find RIP matrices efficiently, which became an active research area.

Meanwhile, back at the ranch, recall that the overhead of channel estimation has been a concern in the millimeter wave MIMO setting.
Luckily, physical evidences suggest that millimeter wave channels are in fact sparse in the frequency domain, and some scholars have thus applied compressive sensing techniques to the problem.
One of the first attempt has been Bajwa et.\ al.\ \cite [BHS10], where they argue that the \m {\ell_0}-norm of the channel matrix may be bounded by a constant, and in such settings the Dantzig Selector may be applied.
What they explored was the estimation of single-antenna channel response with respect to time.
The accompanying paper by the same team (Bajwa et.\ al.\ \cite [BHR08]) shows that \m {X} is RIP for overwhelming probability, providing the ground for the former paper.

To illustrate, consider a linear time-invariant channel \m {y\SB {t} =\RB {x \star h} \SB {t} + z\SB {t}}, where \m {h\SB {t}} is the channel's impulse response, \m {y\SB {t}} is the output, and for each time index, signal \m {x\SB {t}} and noise \m {z\SB {t}} are both i.i.d.
Simply assuming \m {h} is sparse, we may express the convolution between \m {h} with \m {x} by a matrix \m {X}, so that \m {\V {y} =\M {X} \V {h} +\V {z}} for some \m {\M {X}}.
Then \cite [BHR08] showed that \m {\M {X}} is RIP of overwhelmingly probability, and DS is ready to be applied, with some nice performance bounds drawing directly from \cite [CaT07].

\stopsection
\startsection [title={Orthogonal Matching Pursuit}]

Around that time, Tropp and Gilbert \cite [TrG07b] suggest that a greedy algorithm called Orthogonal Matching Pursuit (OMP), which also makes use of an i.i.d.\ random matrix, may also reconstruct the sparse signal sufficiently well in an overwhemingly probability.

Still consider \Rf {y:y:Qx:xz}.
We pick up the columns of \m {\M {Q}} greedily, hoping to correspond to the nonzero components of \m {\V {x}}, and thus recovering the original signal.

\Result
{Algorithm}
{
\startitemize[n]
\item Input \m {\M {Q} \in \MB {R} ^{N_m, N_p}} and \m {\V {y} \in \MB {R} ^{N_m}}, \m {\eta >0}.
%
\item Initialize
%
\Disp {
\NC \V {r}
\LA \NC \V {y} \NR
%
\NC S
\LA \NC \varnothing \NR
}
\item Start the loop with counter \m {i \LA 1}.
\item Find
%
\DispNum {c:c:i0:ir} {
\NC c
\LA \NC \underset {i =0, \dots, N_p-1} {\Rm {argmax}}
\Nm {\M {Q} _{\SB {:,i}} \V {r}} \NR
}
%
and insert \m {S \LA S \cup {i}}.
\item Compute
%
\DispNum {Q:Q:QS:Qy} {
\NC \M {Q} ^\ddagger
\LA \NC \RB {\M {Q} _{\SB {S}} ^\Adj \M {Q} _{\SB {S}}} ^{-1} \M {Q} _{\SB {S}} ^\Adj \NR
%
\NC \V {r}
\LA \NC \V {y} -\M {Q} _{\SB {S}} ^\Adj \M {Q} ^\ddagger \V {y} \NR
}
\item Break if
%
\DispNum {r:2:h':h'} {
\NC \VNm {\V {r}} _2
<\NC \eta \NR
}
%
where \m {\VNm {\M {Q} ^\ddagger} _1} denotes the operator norm.
\item Output 
%
\DispNum {g:g:Qy:Qy} {
\NC \Hat {\V {g}}
\LA \NC \M {Q} ^\ddagger \V {y} \NR
}
\stopitemize
}
%
Here \m {\V {r}} can be thought of as the estimated noise, and \m {S} the estimated position where \m {\V {x}} is nonzero (where we have slightly abused the usage of subscript to indicate position indices).
From Tropp and Gilbert \cite [TrG07a], the probability that OMP recovers \m {\V {x}} completely is overwhelming, providing a perfomance guarantee.


The matrix consisting of a collection of columns that likely spans \m {\V {x}} plays the role of the sensing matrix in the previous section.

\stopsection

\startsection [title={Further Development}]

OMP has since been applied in a variety of ways, and here we restrict our attention to channel estimation.
In fact, ever since, work on channel estimation seems to favored OMP instead of DS, nor other sparse algorithm, without clear justification, and this must have somthing to do with the simplicity of OMP implementation.

Alkhateeb et.\ al.\ \cite [AEL14] proposes an adaptive algorithm with a beam codebook for quantized angles of arrival and departure.
Alkhateeb, Leus, and Heath Jr.\ \cite [ALH15] discusses the trade-off between number of OMP measurement and accuracy in OMP, applied on an all-phase-shifter beamformers, with quantized, non-uniform, predetermined angles.
Gao et.\ al.\ \cite [LGC16] present the jointly reconstruction, with a modified OMP, of several high-dimensional sparse signals with restriction on positions of nonzero components.
Hu, Wang, and He \cite [HWH13] applies OMP to estimate quantized path delay of each OFDM subcarrier, assuming the channel gain in each path is stationary.
Lee, Gil, and Lee \cite [LGL16] considers a hybrid system similar to this paper, with the effective beamformer considered as sensing matrix, where they specially designed a set of quantized angles, and in their simulations permuted DFT matrix is used in analog stage.

Some further attempts include Manoj and Kannu \cite [MaK17], which formulate the sparse condition as overlapping block in the reconstructed signal, and encode possible blocks.
Gurbuz, Yapici, and Guvenc \cite [GYG18] introduce perturbation of the quantized set of angles, on which the sensing matrix is based.
Most recently, Panayirci et.\ al.\ \cite [PAU19] put forward a more thorough treatment, where in addition to application of OMP, they use a first stage of max-likelihood estimation of model parameters, and a second stage of maximum-a-posteriori estimation of channel gains.

Performance guarantee of OMP is also studied, by refining or proposing different conditions.
Cai, Wang, and Xu \cite [CWX10], under assumption on low coherence of columns of matrices, give new bound on OMP.
Coherence-based assumptions, instead of almost-unitarity, make the condition easier to verify.
Cai and Wang \cite [CaW11] extends the investigation to DS and other convex programs for sparse recovery, and Ben-Haim et.\ al.\ \cite [BEE10] follow up and refine their bounds, and conclude that OMP is better for low-SNR scenario, and DS is better for high-SNR.

\stopsection

\startsection [title={Our Contribution}]

With all these efforts, current literature of compressive channel estimation, in our view, has not satisfyingly responded several aspects.
One notable drawback of OMP, as pointed out by \cite [GYG18], is that the quantized angle of the sensing matrix used dos not necessarily match the true ones.
At least in the uniform linear array setting, the angular domain can be used Fourier as sensing matrix, which researchers have correctly noted.
But the true angle of arrival and departure is often slightly off.
We can always use finer quantization, but that leads to higher complexity.
Moreover, researchers usually assume that the channel matrix has small norm, but further justifications and explicit bounds, depending channel parameters (say on sparsity and dimension), may be desired.

In this paper, we argue in favor of DS, rather than OMP, to be another viable solution that has been ignored.
To elaborate, we consider a system model where both digital and analog beamformer are i.i.d.\ random matrices, so that the effective beamformer serves readily as the sensing matrix with RIP.
Assuming uniform linear array, we apply a convex program inspired by DS so as to directly estimate the channel.
We shall see that a modified DS can be done on the angular domain instead of the spatial domain, and for complex numbers instead of real numbers.
Then along the lines of \cite [CaT07], we shall give a bound on expected square error, depending explicitly on the sparsity of the virtual channel matrix and the number of paths of the channel, which will surely future designers, in which the 
Furthermore, it holds under an overwhelming probability, which is also bounded quantitatively.

Lastly, we cast the convex probram to be a second order cone problem to facilitate efficient code.
We claim that, in view of numerical results, DS is often superior to OMP among other methods in terms of performance, but takes higher complexity.


\stopsection

\stopchapter

